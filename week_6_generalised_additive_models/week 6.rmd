---
title: "R Notebook"
output: html_notebook
---

The [R plugin](https://www.jetbrains.com/help/pycharm/r-plugin-support.html) for IntelliJ-based IDEs provides
handy capabilities to work with the [R Markdown](https://www.jetbrains.com/help/pycharm/r-markdown.html) files.
To [add](https://www.jetbrains.com/help/pycharm/r-markdown.html#add-code-chunk) a new R chunk,
position the caret at any line or the code chunk, then click "+".

The code chunk appears:
```{r}
library(ISLR)
library(splines)
library(gam)

data("Wage")
attach(Wage)

gam1<-lm(wage~ns(year,4)+ns(age,5)+education, data=Wage)
par(mfrow=c(1,3))

plot.Gam(gam1, se=TRUE)
```

Type any R code in the chunk, for example:
```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.m3=gam(wage~s(year,4)+s(age,5)+education, data=Wage)
par(mfrow=c(1,3))
plot(gam.m3,se=TRUE,col="blue")
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.m3=gam(wage~s(year,4)+s(age,5)+education, data=Wage)
summary(gam.m3)
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.m3=gam(wage~s(year,4)+s(age,5)+education, data=Wage)
preds=predict(gam.m3,newdata=Wage)
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.lo<-gam(wage~s(year,df=4)+lo(age,span=0.7)+education, data=Wage)
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.lo.i<-gam(wage~lo(year,age,span=0.5)+education, data=Wage)
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.lo.i<-gam(wage~lo(year,age,span=0.5)+education, data=Wage)

library(akima)
par(mfrow=c(1,2))
plot(gam.lo.i,se=TRUE)
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.lr<-gam(I(wage>250)~year+s(age, df=5)+education,family=binomial,data=Wage)
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

table(education, I(wage>250))

##
## education         FALSE TRUE
##  1. < HS Grad       268    0
##  2. HS Grad         966    5
##  3. Some College    643    7
##  4. College Grad    663   22
##  5. Advanced Degree 381   45
```


```{r}
library(ISLR)
library(gam)

data("Wage")
attach(Wage)

gam.lr.s=gam(I(wage>250)~year+s(age,df=5)+education, family=binomial,
             data=Wage, subset=(education!="1. < HS Grad"))
par(mfrow=c(1,3))
plot(gam.lr.s, se=T, col="green")
```

Activity in R: Model Comparison

In the pervious examples the function of yearyear looks rather linear. Perform a series of ANOVA tests in order to determine which of these three models is best:

(a) M1M1​ a GAM that excludes yearyear;

(b) M2M2​ a GAM that uses a linear function of yearyear;

(c) M3M3​ a GAM that uses spline function of yearyear.

Note that each model should also include the ageage and educationeducation variables as previously defined.
```{r}
library(ISLR)
library(gam)
library(splines)
library(foreach)

data("Wage")
attach(Wage)

gam.m1<-gam(wage~s(age,5)+education, data=Wage)
gam.m2<-gam(wage~year+s(age,5)+education,data=Wage)
gam.m3<-gam(wage~s(year,4)+s(age,5)+education, data=Wage)
anova(gam.m1, gam.m2, gam.m3, test="F")
```

# Solution:
gam.m1<-gam(wage~s(age,5)+education, data=Wage)
gam.m2<-gam(wage~year+s(age,5)+education,data=Wage)
gam.m3<-gam(wage~s(year,4)+s(age,5)+education, data=Wage)
anova(gam.m1, gam.m2, gam.m3, test="F")

We find that there is compelling evidence that a GAM with a linear function of yearyear is better than a GAM that does not include yearyear at all. However, there is no evidence that a non-linear function of yearyear is needed.

# Activity in R: Generalised Additive Models
This question relates to the CollegeCollege data set form the library ISLRISLR.
(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.
(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.
(c) Evaluate the model obtained on the test set and explain the results obtained.
(d) For which variables, if any, is there evidence of a non-linear relationship with the response?

```{r}
#a
library("ISLR")
library("leaps")
library("glmnet")
library("gam")

set.seed(0)

dataset_part <- sample(1:3, nrow(College), replace = T, prob = c(0.5, 0.25, 0.25))

p <- ncol(College) - 1
regfit.forward <- regsubsets(Outstate ~ ., data = College[dataset_part == 1, ], nvmax = p, method = "forward")
print(summary(regfit.forward))

reg.summary <- summary(regfit.forward)

validation.mat <- model.matrix(Outstate ~ ., data = College[dataset_part == 2, ])
val.errors <- rep(NA, p)
for (ii in 1:p) {
  coefi <- coef(regfit.forward, id = ii)
  pred <- validation.mat[, names(coefi)] %*% coefi
  val.errors[ii] <- mean((College$Outstate[dataset_part == 2] - pred)^2)
}
k <- which.min(val.errors)
plot(val.errors, xlab = "Number of variables", ylab = "Validation MSE", pch = 19, type = "b")
abline(v = k, col = "red")
grid()

#b
k <- 3
coefi <- coef(regfit.forward, id = k)
coefi

test.mat <- model.matrix(Outstate ~ ., data = College[dataset_part == 3, ])
pred <- test.mat[, names(coefi)] %*% coefi
test.error <- mean((College$Outstate[dataset_part == 3] - pred)^2)
test.error

dataset_part[dataset_part == 2] <- 1
dataset_part[dataset_part == 3] <- 2

gam.model <- gam(Outstate ~ s(Expend, 4) + s(Room.Board, 4) + Private, data = College[dataset_part == 1, ])

par(mfrow = c(1, 3))
plot(gam.model, se = TRUE, col = "blue")

#c
y_hat <- predict(gam.model, newdata = College[dataset_part == 2, ])
MSE <- mean((College[dataset_part == 2, ]$Outstate - y_hat)^2)
MSE

#d
summary(gam.model)
```

Solution:

(a) We first split the given data into three parts: a training set, a validation set and a test set.
library("ISLR")

##
## Attaching package: 'ISLR'
## The following object is masked by '.GlobalEnv':
##
## Wage

library("leaps")
library("glmnet")

## Loading required package: Matrix
## Loaded glmnet 2.0-16

library("gam")

set.seed(0)

# Divide the dataset into three parts: training==1,
# validation==2, and test==3

dataset_part <- sample(1:3, nrow(College), replace = T, prob = c(0.5, 0.25, 0.25))

We then apply the R function regsubsetsregsubsets on the training set to get the best set of predictors for subsets of every size 1≤k≤p1≤k≤p.
p <- ncol(College) - 1
# Fit subsets of various sizes:
regfit.forward <- regsubsets(Outstate ~ ., data = College[dataset_part == 1, ], nvmax = p, method = "forward")
print(summary(regfit.forward))

To determine which subset size kk is the optimal we then compute for each subset size the validation set mean square error.
reg.summary <- summary(regfit.forward)

# Test the trained models on the validation set:
validation.mat <- model.matrix(Outstate ~ ., data = College[dataset_part == 2, ])
val.errors <- rep(NA, p)
for (ii in 1:p) {
    coefi <- coef(regfit.forward, id = ii)
    pred <- validation.mat[, names(coefi)] %*% coefi
    val.errors[ii] <- mean((College$Outstate[dataset_part == 2] - pred)^2)
}
k <- which.min(val.errors)
plot(val.errors, xlab = "Number of variables", ylab = "Validation MSE", pch = 19, type = "b")
abline(v = k, col = "red")
grid()

# Example: Backfitting (Wood (2006))
```{r}
## This code requires inputs x,y,m,edf to work
f<-x*0;
alpha<-mean(y);
ok <- TRUE
while (ok) { # backfitting loop
for (i in 1:m) { # loop through the smooth terms
ep <- y - rowSums(f[,-i]) - alpha
b <- smooth.spline(x[,i],ep,df=edf[i])
f[,i] <- predict(b,x[,i])$y
}
rss <- sum((y-rowSums(f))^2)
if (abs(rss-rss0)<1e-6*rss)
  ok <- FALSE
rss0 <- rss
}
```

Activity in R: Backfitting

You will now explore backfitting in the context of multiple linear regression.

Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following iterative approach: we repeatedly hold all but one coefficient estimate fixed at its current value, and update only that coefficient estimate using simple linear regression. The process is continued until convergence. We now try this out on a toy example.

(a) Generate a response YY and two predictors X1X1​ and X2X2​ , with n=100n=100.

(b) Initialize β^1β^​1​ to take on a value of your choice.

(c) Keeping β^1β^​1​ fixed, fit the model
Y−β^1X1=β0+β2X2+ε.
Y−β^​1​X1​=β0​+β2​X2​+ε.

You can do this as follows:
a =y - beta1 * x1
beta2 = lm( a~x2 )$coef[2]

(d) Keeping β^2β^​2​ fixed, fit the model
Y−β^2X2=β0+β1X1+ε.
Y−β^​2​X2​=β0​+β1​X1​+ε.

```{r}
set.seed(0)

n <- 100

X1 <- rnorm(n)
X2 <- rnorm(n)

# the true values of beta_i:
beta_0 <- 3
beta_1 <- 5
beta_2 <- -0.2

Y <- beta_0 + beta_1 * X1 + beta_2 * X2 + 0.1 * rnorm(n)

beta_1_hat <- -3

n_iters <- 10

beta_0_estimates <- c()
beta_1_estimates <- c()
beta_2_estimates <- c()
for (ii in 1:n_iters) {

  a <- Y - beta_1_hat * X1
  beta_2_hat <- lm(a ~ X2)$coef[2]

  a <- Y - beta_2_hat * X2
  m <- lm(a ~ X1)
  beta_1_hat <- m$coef[2]

  beta_0_hat <- m$coef[1]

  beta_0_estimates <- c(beta_0_estimates, beta_0_hat)
  beta_1_estimates <- c(beta_1_estimates, beta_1_hat)
  beta_2_estimates <- c(beta_2_estimates, beta_2_hat)
}

m <- lm(Y ~ X1 + X2)

old_par <- par(mfrow = c(1, 3))

plot(1:n_iters, beta_0_estimates, main = "beta_0", pch = 19,
     ylim = c(beta_0 * 0.999, max(beta_0_estimates)))
abline(h = beta_0, col = "green", lwd = 4)
abline(h = m$coefficients[1], col = "gray", lwd = 4)
grid()

plot(1:n_iters, beta_1_estimates, main = "beta_1", pch = 19)
abline(h = beta_1, col = "green", lwd = 4)
abline(h = m$coefficients[2], col = "gray", lwd = 4)
grid()

plot(1:n_iters, beta_2_estimates, main = "beta_2", pch = 19)
abline(h = beta_2, col = "green", lwd = 4)
abline(h = m$coefficients[3], col = "gray", lwd = 4)
grid()
```

# Activity in R: Backfitting
You will now explore backfitting in the context of multiple linear regression.

Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following iterative approach: we repeatedly hold all but one coefficient estimate fixed at its current value, and update only that coefficient estimate using simple linear regression. The process is continued until convergence. We now try this out on a toy example.

(a) Generate a response YY and two predictors X1X1​ and X2X2​ , with n=100n=100.

(b) Initialize β^1β^​1​ to take on a value of your choice.

(c) Keeping β^1β^​1​ fixed, fit the model
Y−β^1X1=β0+β2X2+ε.
Y−β^​1​X1​=β0​+β2​X2​+ε.

You can do this as follows:
a =y - beta1 * x1
beta2 = lm( a~x2 )$coef[2]

(d) Keeping β^2β^​2​ fixed, fit the model
Y−β^2X2=β0+β1X1+ε.
Y−β^​2​X2​=β0​+β1​X1​+ε.

```{r}
library(mgcv)
library(gamair)
data(brain)
head(brain, n=2)
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)

m0<-gam(medFPQ~s(Y,X,k=250))
gam.check(m0)
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]
m1<-gam(medFPQ^.25~s(Y,X,k=250),data=brain)
gam.check(m1)
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]
m2<-gam(medFPQ~s(Y,X,k=250),data=brain,family=Gamma(link=log))
gam.check(m2)
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]
m1<-gam(medFPQ^.25~s(Y,X,k=250),data=brain)
m2<-gam(medFPQ~s(Y,X,k=250),data=brain,family=Gamma(link=log))

mean(fitted(m1)^4)
mean(fitted(m2))
mean(brain$medFPQ)
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]
m2<-gam(medFPQ~s(Y,X,k=250),data=brain,family=Gamma(link=log))
m2

vis.gam(m2, plot.type="contour", too.far=0.03,color="gray", n.grid=60,zlim=c(-1,2))

```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]

m3<-gam(medFPQ~s(Y,k=30)+s(X,k=30),data=brain,family=Gamma(link=log))
m3
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]
m3<-gam(medFPQ~s(Y,k=30)+s(X,k=30),data=brain,family=Gamma(link=log))
vis.gam(m3, plot.type="contour", too.far=0.03,color="gray", n.grid=60,zlim=c(-1,2))
```


```{r}
library(mgcv)
library(gamair)
data(brain)
attach(brain)
brain<-brain[medFPQ>5e-3,]

m2<-gam(medFPQ~s(Y,X,k=250),data=brain,family=Gamma(link=log))
m3<-gam(medFPQ~s(Y,k=30)+s(X,k=30),data=brain,family=Gamma(link=log))
AIC(m3,m2)
```

Activity in R: Choice of smoothing basis in mgcv

There is a variety of smoothing basis built into mgcvmgcv. The basis is specified within the model formula, for example: s(,bs="cr")s(,bs="cr"). If not specified default bs="tp"bs="tp" is used.

Familiarise yourself with the help file of the smooth.termssmooth.terms in the mgcvmgcv package.
library(mgcv)
help(smooth.terms)

Match the smoothing basis in R with its description:

(1) tptp (2) tsts (3) crcr (4) cscs (5) cccc (6) psps

(a) TPRS with shrinkage.
(b) cyclic CRS. As CRS but start point the same as an endpoint.
(c) cubic regression spline (CRS). Produces directly interpretable parameters. Can only smooth with respect to one covariate.
(d) thin plate regression splines (TPRS). Can smooth any number of covariates.
(e) CRS with shrinkage.
(f) P-splines.

```{r}
library(mgcv)
help(smooth.terms)
```




























