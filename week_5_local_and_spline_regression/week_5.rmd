---
title: "week 5 R Material"
output: html_document
---


```{r}
xx1 <- seq(from = -1, to = 1, length.out = 100)
yy <- (1-abs(xx1)^3)^3
plot(xx1,yy,xlim = c(-2,2),type='l',xlab='u',ylab='K(u)')
lines(c(-2,-1),c(0,0)); lines(c(1,2),c(0,0))
yy <- 3/4*(1-xx1^2)
lines(xx1,yy,col=3)
xx2 <- seq(from = -2, to = 2, length.out = 200)
yy <- dnorm(xx2)
lines(xx2,yy,col=4)
lines(c(-2,-1,-1,1,1,2),c(0,0,1/2,1/2,0,0),col=5)
lines(c(-2,-1,0,1,2),c(0,0,1,0,0),col=6)
legend("topleft", legend=c("tri-cube", "Epanechnikov", "Gaussian", "Box",
                           "Triangle"),lty=1, col=c(1,3,4,5,6))
```

```{r}
diabetes <- read.table("data/diabetes.dat", header=TRUE, quote="\"")
diabetes <- diabetes[order(diabetes$Age),]
head(diabetes)
##     Age  Base.Deficit C.Peptide
## 15  0.9         -11.6       3.0
## 24  1.0          -8.2       3.9
## 6   1.8         -19.2       3.4
## 10  1.9         -25.0       3.7
## 11  2.2          -3.1       3.9
## 36  4.2         -17.0       5.1
```


```{r}
diabetes <- read.table("data/diabetes.dat", header=TRUE, quote="\"")
diabetes <- diabetes[order(diabetes$Age),]
plot(diabetes$Age,log(diabetes$C.Peptide))
lines(ksmooth(diabetes$Age,log(diabetes$C.Peptide), bandwidth=5.0, kernel="normal"))
lines(ksmooth(diabetes$Age,log(diabetes$C.Peptide), bandwidth=2.0, kernel="normal"),
      lty=2)
title("Kernel smooth")
legend("topleft",c("bandwidth=5", "bandwidth=2"),lty=1:2)
```

```{r}
trade.union <- read.table("data/tradeunion.dat", header = T)
attach(trade.union)
require(locfit)

# locfit 1.5-9.4 	 2020-03-24

fit <- locfit(union.member~lp(wage, nn=0.7, deg=2), data=trade.union, family="binomial")
plot(fit,ylim = c(0,1),type='n',main="Trade Union Data")
newxx <- seq(from = min(wage), to = max(wage), length.out = 100)
newdat <- data.frame(wage=newxx)
ypred <- predict(fit, newdat, se.fit = T)
lines(newxx,ypred$fit)
lines(newxx,ypred$fit-ypred$se.fit,lty=2)
lines(newxx,ypred$fit+ypred$se.fit,lty=2)
points(jitter(wage), union.member, main = "trade union data set", pch = 3)
```

trade.union <- read.table("/course/data/tradeunion.dat", header = T)
attach(trade.union)
require(locfit)

# locfit 1.5-9.4 	 2020-03-24

fit=locfit(union.member~lp(wage,nn=0.7,deg=2), data=trade.union,family="binomial")
plot(fit,ylim = c(0,1),type='n',main="Trade Union Data")
newxx=seq(from = min(wage),to = max(wage),length.out = 100)
newdat=data.frame(wage=newxx)
ypred=predict(fit,newdat,se.fit = T)
lines(newxx,ypred$fit)
lines(newxx,ypred$fit-ypred$se.fit,lty=2)
lines(newxx,ypred$fit+ypred$se.fit,lty=2)
points(jitter(wage), union.member, main = "trade union data set", pch = 3)

```{r}

```

Solution: Let us first load and inspect the data:
```{r}
ethanol<-read.table("data/ethanol.txt",header=T)
print(dim(ethanol))
head(ethanol)
```

A scatterplot of `NOx` against `E` can be generated by:
```{r}
attach(ethanol)
plot(E, NOx)

Esq <- E*E
ethanol.lm <- lm(NOx~E+Esq)
Egrid <- seq(from=min(E),to=max(E),length=50)
new.data <- data.frame(E=Egrid,Esq=Egrid*Egrid)
ethanol.pred<-predict(ethanol.lm,newdata=new.data)
lines(Egrid,ethanol.pred,col=2)
lines(loess.smooth(E,NOx,span=0.2),lty=2,col=3)
```

From the above figure, it seems that loess smooth capture more bell-shaped structure of the data. This is slightly different from the quadratic regression fit.

```{r}
ethanol<-read.table("data/ethanol.txt", header=T)
print(dim(ethanol))

head(ethanol)

attach(ethanol)
plot(E, NOx)
Esq <- E*E
ethanol.lm<-lm(NOx~E+Esq)
Egrid<-seq(from=min(E),to=max(E),length=50)
new.data<-data.frame(E=Egrid,Esq=Egrid*Egrid)
ethanol.pred<-predict(ethanol.lm,newdata=new.data)
lines(Egrid,ethanol.pred,col=2)
lines(loess.smooth(E,NOx,span=0.2),lty=2,col=3)

```

# Activity in R: Kernel Smoother
Download the data set `air` (there are four columns in this data frame, ozone, radiation, temperature and wind). The data set records New York Air Quality Measurements, May to September 1973.

Plot a scatterplot of `ozone` against `radiation` and superimpose loess and kernel smooths on the scatterplot. You may need to change the default level of smoothing for the kernel and loess smooths.

```{r}
air<-read.table("data/air.txt", header=T)
attach(air)
plot(radiation,ozone)
lines(loess.smooth(radiation,ozone),col=2)
lines(ksmooth(radiation,ozone,bandwidth=100),lty=2,col=3)
```

```{r}
air<-read.table("data/air.txt", header=T)
attach(air)
plot(radiation,ozone)
lines(loess.smooth(radiation,ozone),col=2)
lines(ksmooth(radiation,ozone,bandwidth=100),lty=2,col=3)
```

# example: polynomial regression
```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit <- lm(wage~poly(age, 4, raw=T), data=Wage)
coef(summary(fit))
```

There are several other equivalent ways of fitting this model:
```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit2 <- lm(wage~age+I(age^2)+I(age^3)+I(age^4), data=Wage)
coef(summary(fit2))
```

or
```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit3 <- lm(wage~cbind(age, age^2, age^3, age^4), data=Wage)
```

Let us now fit the degree-4 polynomial to wage as a function of age in the WageWage dataset

```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit <- lm(wage~poly(age, 4, raw=T), data=Wage)
attach(Wage)
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata=list(age=age.grid), se=TRUE)
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age,wage,xlim=agelims, cex=.5,col="darkgrey")
title("Degree-4 Polynomial", outer=T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1,col="blue",lty=4)
```


```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit.1 <- lm(wage~age, data=Wage)
fit.2 <- lm(wage~poly(age, 2), data=Wage)
fit.3 <- lm(wage~poly(age, 3), data=Wage)
fit.4 <- lm(wage~poly(age, 4), data=Wage)
fit.5 <- lm(wage~poly(age, 5), data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
```

# example: polynomial logistic regression
```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit <- glm(I(wage>250)~poly(age, 4), data=Wage, family=binomial)

agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])

preds <- predict(fit, newdata=list(age=age.grid), se=T)
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands <- exp(se.bands.logit)/(1+exp(se.bands.logit))
```


```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit <- glm(I(wage>250)~poly(age, 4), data=Wage, family=binomial)
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata=list(age=age.grid), se=T)
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands <- exp(se.bands.logit)/(1+exp(se.bands.logit))

preds <- predict(fit, newdata=list(age=age.grid), type="response", se=T)
```


```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit <- glm(I(wage>250)~poly(age, 4), data=Wage, family=binomial)
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata=list(age=age.grid), se=T)
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands <- exp(se.bands.logit)/(1+exp(se.bands.logit))

preds <- predict(fit, newdata=list(age=age.grid), type="response", se=T)

attach(Wage)

## The following objects are masked from Wage (pos = 4):
##
## age, education, health, health ins, jobclass, logwage, maritl,
## race, region, sex, wage, X, year

plot(age, I(wage>250),xlim=agelims, type="n", ylim=c(0,0.2))
points(jitter(age), I((wage>250)/5), cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=4)
```


```{r}
library(ISLR)
data("Wage")
attach(Wage)

table(cut(age,4))

fit <- lm(wage~cut(age, 4), data=Wage)
coef(summary(fit))
```

# Activity in R: Polynomial regression
In this activity, you will further analyse the WageWage data set considered in this section. Perform polynomial regression to predict wagewage using ageage. Use cross-validation to select the optimal degree dd for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.

Solution: First we will perform polynomial regression for various polynomial degrees:
```{r}
library(boot)
set.seed(1)
cv.error <- rep(0, 10)
for( i in 1:10 ){
  glm.fit <- glm(wage ~ poly(age, i), data=Wage )
  cv.error[i] <- cv.glm(Wage, glm.fit, K=10 )$delta[1]
}
```

And plot the cross-validation errors:

```{r}
plot( 1:10, cv.error, pch=19, type='b', xlab='degree of polynomial',
      ylab='CV estimate of the prediction error' )
```
The minimal CV-error corresponds to the degree of the polynomial equal to 9:

```{r}
me <- which.min(cv.error )
me
```

Hence:
```{r}
m <- glm(wage ~ poly(age, me), data=Wage )
```

The polynomial fit plot can be obtained as follows:
```{r}
plot( Wage$age, Wage$wage )
aRng <- range(Wage$age)
a_predict <- seq(from=aRng[1], to=aRng[2], length.out=100 )
w_predict <- predict(m, newdata=list(age=a_predict ) )
lines( a_predict, w_predict, col='red' )
```


```{r}
library(ISLR)
library(boot)
data("Wage")
attach(Wage)

set.seed(1)
cv.error <- rep(0, 10)
for( i in 1:10 ){
  glm.fit <- glm(wage ~ poly(age, i), data=Wage )
  cv.error[i] <- cv.glm(Wage, glm.fit, K=10 )$delta[1]
}

plot( 1:10, cv.error, pch=19, type='b', xlab='degree of polynomial', ylab='CV estimate of the prediction error' )

me = which.min( cv.error )
me

m <- glm(wage ~ poly(age, me), data=Wage )

plot( Wage$age, Wage$wage )
aRng <- range(Wage$age)
a_predict <- seq(from=aRng[1], to=aRng[2], length.out=100 )
w_predict <- predict(m, newdata=list(age=a_predict ) )
lines( a_predict, w_predict, col='red' )
```

# Example:
A linear spline with two knots (red) and 50 knots (green):
```{r}
library(SemiPar)
require(splines)

## Loading required package: splines

data(lidar)
attach(lidar)
fit <- lm(logratio ~ bs(x = range, knots = c(550,600), degree = 1))
fit2<- lm(logratio ~ bs(x = range,df = 51, degree = 1))
plot(lidar, main = "LIDAR data")
xpred <- seq(400,700, length.out = 200)
lines(xpred, predict(fit, data.frame(range = xpred)),col=2,lwd=3)
lines(xpred, predict(fit2, data.frame(range = xpred)),col=3,lty=2,lwd=3)
```

# Example:
natural spline (red) and ordinary spline (blue)

```{r}
library(ISLR)
attach(Wage)
library(splines)
agelims=range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
fit <- lm(wage~ns(age, df=4), data=Wage)
fit2 <- lm(wage~bs(age, df=4), data=Wage)
pred <- predict(fit, newdata=list(age=age.grid), se=T)
pred2 <- predict(fit2, newdata=list(age=age.grid), se=T)
plot(age,wage,col="grey")
lines(age.grid,pred$fit,col="red",lwd=2)
lines(age.grid,pred2$fit,col="blue", lwd=2)
```


```{r}
library(MASS)
m <- lm(nox ~ poly(dis, 3), data=Boston )
summary(m)

plot( Boston$dis, Boston$nox, xlab='dis', ylab='nox', main='third degree polynomial fit' )
dis_range <- range(Boston$dis )
dis_samples <- seq(from=dis_range[1], to=dis_range[2], length.out=100 )
y_hat <- predict(m, newdata=list(dis=dis_samples ) )

lines( dis_samples, y_hat, col='red' )
```


```{r}
library(MASS)
d_max <- 10
# The training RSS:
training_rss <- rep(NA, d_max)
for( d in 1:d_max ){
  m <- lm(nox ~ poly(dis, d), data=Boston )
  training_rss[d] <- sum(( m$residuals )^2 )
}
```


```{r}
library(MASS)
set.seed(1)
k <- 10
d_max <- 10
folds <- sample(1:k, nrow(Boston), replace=TRUE )
cv.rss.test <- matrix(NA, k, d_max )
cv.rss.train <- matrix(NA, k, d_max )

for( d in 1:d_max ){
  for( fi in 1:k ){ # for each fold
    fit <- lm(nox ~ poly(dis, d), data=Boston[folds!=fi,] )

    y_hat <- predict(fit, newdata=Boston[folds!=fi,] )
    cv.rss.train[fi,d] <- sum(( Boston[folds!=fi,]$nox - y_hat )^2 )

    y_hat <- predict(fit, newdata=Boston[folds==fi,] )
    cv.rss.test[fi,d] <- sum(( Boston[folds==fi,]$nox - y_hat )^2 )
  }
}

cv.rss.train.mean = apply(cv.rss.train,2,mean)
cv.rss.train.stderr <- apply(cv.rss.train, 2, sd)/sqrt(k)

cv.rss.test.mean <- apply(cv.rss.test, 2, mean)
cv.rss.test.stderr <- apply(cv.rss.test, 2, sd)/sqrt(k)

min_value <- min(c(cv.rss.test.mean, cv.rss.train.mean) )
max_value <- max(c(cv.rss.test.mean, cv.rss.train.mean) )

plot( 1:d_max, cv.rss.train.mean, xlab='polynomial degree', ylab='RSS', col='red', pch=19, type='b', ylim=c(min_value,max_value) )
lines( 1:d_max, cv.rss.test.mean, col='green', pch=19, type='b' )
legend( "topright", legend=c("train RSS","test RSS"), col=c("red","green"), lty=1, lwd=2 )
```


```{r}
library(MASS)
library(splines)
m <- lm(nox ~ bs(dis, df=4), data=Boston )
plot( Boston$dis, Boston$nox, xlab='dis', ylab='nox', main='bs with df=4 fit' )

dis_range <- range(Boston$dis )
dis_samples <- seq(from=dis_range[1], to=dis_range[2], length.out=100 )
y_hat <- predict(m, newdata=list(dis=dis_samples ) )
lines( dis_samples, y_hat, col='red' )
```



```{r}
library(MASS)
library(splines)
set.seed(1)
dof_choices <- c(3:15)
n_dof_choices <- length(dof_choices)

# The RSS estimated using cross-validation:

k <- 5
folds <- sample(1:k, nrow(Boston), replace=TRUE )
cv.rss.test <- matrix(NA, k, n_dof_choices )
cv.rss.train <- matrix(NA, k, n_dof_choices )

for( di in 1:n_dof_choices ){
   for( fi in 1:k ){ # for each fold
      fit <- lm(nox ~ bs(dis, df=dof_choices[di]), data=Boston[folds!=fi,] )

      y_hat <- predict(fit, newdata=Boston[folds!=fi,] )
      cv.rss.train[fi,di] <- sum(( Boston[folds!=fi,]$nox - y_hat )^2 )

      y_hat <- predict(fit, newdata=Boston[folds==fi,] )
      cv.rss.test[fi,di] <- sum(( Boston[folds==fi,]$nox - y_hat )^2 )
   }
}

cv.rss.train.mean <- apply(cv.rss.train, 2, mean)
cv.rss.train.stderr <- apply(cv.rss.train, 2, sd)/sqrt(k)

cv.rss.test.mean <- apply(cv.rss.test, 2, mean)
cv.rss.test.stderr <- apply(cv.rss.test, 2, sd)/sqrt(k)

min_value <- min(c(cv.rss.test.mean, cv.rss.train.mean) )
max_value <- max(c(cv.rss.test.mean, cv.rss.train.mean) )

plot( dof_choices, cv.rss.train.mean, xlab='spline d.o.f.', ylab='Train-RSS', col='red', pch=19, type='b' )

plot( dof_choices, cv.rss.test.mean, xlab='spline d.o.f.', ylab='Test-RSS', col='red', pch=19, type='b' )

```

Example: Generalized Cross-Validation
```{r}
fossil <- read.table("data/fossil.dat",header=T)
plot(fossil,pch=1, main="smoothed fossil data")
ss1 <- smooth.spline(fossil$age, fossil$strontium.ratio)
ss2 <- smooth.spline(fossil$age, fossil$strontium.ratio, df = 4)
ss3 <- smooth.spline(fossil$age, fossil$strontium.ratio, df = 25)
xx <- seq(90, 130, length.out = 200)
lines(predict(ss1,xx), col=2)
lines(predict(ss2,xx), col=2,lty=2)
lines(predict(ss3,xx), col=2,lty=3)
```


```{r}
fossil <- read.table("data/fossil.dat",header=T)
num.df <- 101
df.grid<-seq(8,20,length=num.df)
GCV <- rep(NA,num.df)
n <- length(fossil$age)
for (i in 1:num.df)
{
 fit    <- smooth.spline(fossil$strontium.ratio~fossil$age,df=df.grid[i])
 RSS    <- sum((fossil$strontium.ratio-predict(fit,fossil$age)$y)^2)
 GCV[i] <- RSS/(1-df.grid[i]/n)^2
}

plot(df.grid,GCV,type="l",xlab="degree of freedom", ylab="GCV")

ind.min <- order(GCV)[1]
lines(rep(df.grid[ind.min],2),c(min(GCV),max(GCV)))
```


```{r}
fossil <- read.table("data/fossil.dat",header=T)
sp<-smooth.spline(fossil$age,fossil$strontium.ratio,cv=FALSE)
sp$df
```

--------------------------------------------------------
# Activity in R: Smoothing spline
Recall the WageWage data set in the library `ISLR`. Visualise ageage vs wagewage using a scatterplot. Fit a smoothing spline using the `smooth.spline()` function in R. Compare two methods of fitting the smoothing spline: one of them is by providing `df=16` and the second one by using cross-validation `cv=TRUE`. Add the visualisation of both fitted splines onto the scatterplot. Explain how the fit is obtained in both cases.

# Solution:
```{r}
library(ISLR)
attach(Wage)
agelims <- range(age)
plot(age,wage,xlim=agelims, cex=.5,col="darkgrey")
title("Smoothing spline")
fit=smooth.spline(age,wage,df=16)
fit2 <- smooth.spline(age, wage, cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
```

In the first call to `smooth.spline()`, we specified `df=16`. The function then determines which `λ` leads to `16` degrees of freedom. In the second case, we select the smoothness level by cross-validation, and this results in a value of `λ` that yields `df=6.8`.

```{r}
require(mgcv)

galaxy <-read.delim("data/galaxy.data", sep=",")

fit <- gam(formula = velocity~s(east.west, north.south), data = galaxy)
plot(fit)
```



```{r}
galaxy <-read.delim("data/galaxy.data", sep=",")
require(mgcv)
loc.lin.reg <- loess(formula = velocity ~ north.south +
                      east.west, data = galaxy, span = 0.7, degree = 2,
                     normalize = T, family = "gaussian")
attach(galaxy)

xgrid <- seq(min(north.south), max(north.south), length.out = 50)
ygrid <- seq(min(east.west), max(east.west), length.out = 50)

new.coords <- matrix(data = NA, nrow = 2500, ncol = 2)
new.coords[, 1] <- as.vector(outer(rep(1, 50), xgrid))
new.coords[, 2] <- as.vector(outer(ygrid, rep(1, 50)))

new.coords <- as.data.frame(new.coords)
names(new.coords) <- c("north.south", "east.west")

Z <- predict(loc.lin.reg, newdata = new.coords)
Z <- matrix(data = Z, ncol = 50)
library(lattice)
wireframe(Z, screen = list(z = 170, x = -60))
```



```{r}
galaxy <-read.delim("data/galaxy.data", sep=",")
require(mgcv)
loc.lin.reg <- loess(formula = velocity ~ north.south +
                      east.west, data = galaxy, span = 0.7, degree = 2,
                     normalize = T, family = "gaussian")
attach(galaxy)

xgrid <- seq(min(north.south), max(north.south), length.out = 50)
ygrid <- seq(min(east.west), max(east.west), length.out = 50)

new.coords <- matrix(data = NA, nrow = 2500, ncol = 2)
new.coords[, 1] <- as.vector(outer(rep(1, 50), xgrid))
new.coords[, 2] <- as.vector(outer(ygrid, rep(1, 50)))

new.coords <- as.data.frame(new.coords)
names(new.coords) <- c("north.south", "east.west")

Z <- predict(loc.lin.reg, newdata = new.coords)
Z <- matrix(data = Z, ncol = 50)
library(lattice)
wireframe(Z, screen = list(z = 170, x = -60))
```

--------------------------------------------
# Exercise 1
In this exercise we follow the Lab notes from Section 7.8 of James et al. (2013), i.e. the ISLR book. We will skip Section 7.8.1 on polynomial regression and step functions but it is recommended to read through this section (combined with the course notes). Instead we focus on section 7.8.2 to learn how to fit regression splines, natural splines and smoothing splines.

We will focus on the WageWage dataset available in the `R` package `ISLR`.

```{r}
library(ISLR)
data("Wage")
attach(Wage)

fit <- lm(wage~bs(age, knots=c(25,40,60)), data=Wage)

age.grid <- seq(from=min(age), to=max(age), length=100)
pred <- predict(fit, newdata=list(age=age.grid), se=TRUE)

plot(age, wage, col="gray")
lines(age.grid, pred$fit, lwd=2)
lines(age.grid, pred$fit + 2*pred$se, lty=2)
lines(age.grid, pred$fit - 2*pred$se, lty=2)
```


```{r}
library(ISLR)
data("Wage")
attach(Wage)
```

1. Consider a spline regression to fit `wage` to `age` considering knots
at 25, 40 and 60. Represent the estimated regression line for a sequence of age values of length 100 ranging from the minimum to the maximum observed values. Add a confidence interval using `±2` standard error.

# Q1
```{r}
fit <- lm(wage~bs(age, knots=c(25,40,60)), data=Wage)

age.grid <- seq(from=min(age), to=max(age), length=100)
pred <- predict(fit, newdata=list(age=age.grid), se=TRUE)

plot(age, wage, col="gray")
lines(age.grid, pred$fit, lwd=2)
lines(age.grid, pred$fit + 2*pred$se, lty=2)
lines(age.grid, pred$fit - 2*pred$se, lty=2)
```

2. How many basis function were used in the previous question? Now, instead
of specifying the locations of the knots, use the `df` and `degree` arguments to specify a quadratic spline regression with 6 degrees of freedom. Fit the model and display it as in 1). Where were the knots located?

# Q2
```{r}
fit2 <- lm(wage~bs(age, df=6, degree=2), data=Wage)
attr(bs(age, df=6), "knots", degree=2)
# 20% 40% 60% 80%
#  32  39  46  53
pred2 <- predict(fit2, newdata=list(age=age.grid), se=TRUE)

plot(age, wage, col="gray")
lines(age.grid, pred2$fit, lwd=2)
lines(age.grid, pred2$fit + 2*pred2$se, lty=2)
lines(age.grid, pred2$fit - 2*pred2$se, lty=2)
```

3. Use the `ns()` function to fit a natural cubic spline with 6 degree of
freedom. Superimpose it with the spline regression from 1). What is the main comment that you can make when comparing both models?

# Q3
```{r}
fit3 <- lm(wage~ns(age, df=6), data=Wage)
pred3 <- predict(fit3, newdata=list(age=age.grid), se=TRUE)

plot(age, wage, col="gray")
lines(age.grid, pred$fit, lwd=2)
lines(age.grid, pred$fit + 2*pred$se, lty=2)
lines(age.grid, pred$fit - 2*pred$se, lty=2)
lines(age.grid, pred3$fit, lwd=2, col="red")
lines(age.grid, pred3$fit + 2*pred3$se, lty=2, col="red")
lines(age.grid, pred3$fit - 2*pred3$se, lty=2, col="red")
```

4. Use the `smooth.spline()` function to fit a smoothing spline to the data.
In the first place use the argument `df=16` allowing for a 16 degrees of freedom. In a second step, consider instead the argument `cv=TRUE` to use cross-validation to select the optimal degree of freedom. Display both fitted lines. What do the respective models do?

# Q4
```{r}
plot(age, wage, xlim=range(age), cex=0.5, col="darkgrey")
title("Smoothing Spline")
fit4 <- smooth.spline(age, wage, df=16)
fit5 <- smooth.spline(age, wage, cv=TRUE)
fit5$df
lines(fit4, col="red", lwd=2)
lines(fit5, col="blue", lwd=2)
```

