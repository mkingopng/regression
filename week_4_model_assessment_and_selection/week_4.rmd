---
title: "Week 4 example notebook"
output: html_notebook
---

```{r}
library(faraway)
data("cheddar")
```


# LOO-CV error curves for cheddar

```{r}
N <- nrow(cheddar)
MSE_ace <- MSE_h2s <- MSE_lac <- MSE_3 <- vector(length=N)
MSE_ace_h2s <- MSE_ace_lac <- MSE_h2s_lac <- Mean_3 <- vector(length=N)

for(i in 1:N){
  train_set <- cheddar[-i,]
  vali_set <- cheddar[i,]

  res_lm_ace <- lm(taste ~ Acetic, data=train_set)
  res_lm_h2s <- lm(taste ~ H2S, data=train_set)
  res_lm_lac <- lm(taste ~ Lactic, data=train_set)
  res_lm2_ace_h2s <- lm(taste ~ Acetic + H2S, data=train_set)
  res_lm2_ace_lac <- lm(taste ~ Acetic + Lactic, data=train_set)
  res_lm2_h2s_lac <- lm(taste ~ H2S + Lactic, data=train_set)
  res_lm3_train <- lm(taste ~ Acetic + H2S + Lactic, data=train_set)

  pred_ace <- predict(res_lm_ace,vali_set)
  pred_h2s <- predict(res_lm_h2s,vali_set)
  pred_lac <- predict(res_lm_lac,vali_set)

  pred_ace_h2s <- predict(res_lm2_ace_h2s,vali_set)
  pred_ace_lac <- predict(res_lm2_ace_lac,vali_set)
  pred_h2s_lac <- predict(res_lm2_h2s_lac,vali_set)

  pred_lm3 <- predict(res_lm3_train,vali_set)

  MSE_ace[i] <- (vali_set[,1] - pred_ace)^2
  MSE_h2s[i] <- (vali_set[,1] - pred_h2s)^2
  MSE_lac[i] <- (vali_set[,1] - pred_lac)^2

  MSE_ace_h2s[i] <- (vali_set[,1] - pred_ace_h2s)^2
  MSE_ace_lac[i] <- (vali_set[,1] - pred_ace_lac)^2
  MSE_h2s_lac[i] <- (vali_set[,1] - pred_h2s_lac)^2

  MSE_3[i] <- (vali_set[,1] - pred_lm3)^2
}

models <- c("ace", "h2s", "lac", "ace_h2s", "ace_lac","h2s_lac", "3")

plot(c(mean(MSE_ace), mean(MSE_h2s), mean(MSE_lac),
       mean(MSE_ace_h2s), mean(MSE_ace_lac), mean(MSE_h2s_lac),
       mean(MSE_3)),
       xaxt = "n", xlab="Models", ylab="MSE", col="red", type="b", pch=16)
axis(1, at=1:7, labels=models)
```

# K-folds CV error curves for cheddar

```{r}
N <- nrow(cheddar)
m <- 5
nrep <- 10
MSE_mCV_ace <- MSE_mCV_h2s <- MSE_mCV_lac  <- matrix(nrow=nrep, ncol=m)
MSE_mCV_ace_h2s <- MSE_mCV_ace_lac <- matrix(nrow=nrep, ncol=m)
MSE_mCV_h2s_lac <- MSE_mCV_3 <- matrix(nrow=nrep, ncol=m)

for(i in 1:nrep){

  set.seed(122+i)
  shuffle_id <- sample(1:N, size=N, replace=F)
  cheddar_new <- cheddar[shuffle_id,]
  seq_id <- round(seq(0, N, by=N/m), 0)

  for(j in 1:m){
    test_id <- (seq_id[j]+1):seq_id[j+1]
    train_set <- cheddar_new[-test_id,]
    vali_set <- cheddar_new[test_id,]

    res_lm_ace <- lm(taste ~ Acetic, data=train_set)
    res_lm_h2s <- lm(taste ~ H2S, data=train_set)
    res_lm_lac <- lm(taste ~ Lactic, data=train_set)
    res_lm2_ace_h2s <- lm(taste ~ Acetic + H2S, data=train_set)
    res_lm2_ace_lac <- lm(taste ~ Acetic + Lactic, data=train_set)
    res_lm2_h2s_lac <- lm(taste ~ H2S + Lactic, data=train_set)
    res_lm3_train <- lm(taste ~ Acetic + H2S + Lactic, data=train_set)

    pred_ace <- predict(res_lm_ace,vali_set)
    pred_h2s <- predict(res_lm_h2s,vali_set)
    pred_lac <- predict(res_lm_lac,vali_set)

    pred_ace_h2s <- predict(res_lm2_ace_h2s,vali_set)
    pred_ace_lac <- predict(res_lm2_ace_lac,vali_set)
    pred_h2s_lac <- predict(res_lm2_h2s_lac,vali_set)

    pred_lm3 <- predict(res_lm3_train,vali_set)

    MSE_mCV_ace[i,j] <- mean((vali_set[,1] - pred_ace)^2)
    MSE_mCV_h2s[i,j] <- mean((vali_set[,1] - pred_h2s)^2)
    MSE_mCV_lac[i,j] <- mean((vali_set[,1] - pred_lac)^2)

    MSE_mCV_ace_h2s[i,j] <- mean((vali_set[,1] - pred_ace_h2s)^2)
    MSE_mCV_ace_lac[i,j] <- mean((vali_set[,1] - pred_ace_lac)^2)
    MSE_mCV_h2s_lac[i,j] <- mean((vali_set[,1] - pred_h2s_lac)^2)

    MSE_mCV_3[i,j] <- mean((vali_set[,1] - pred_lm3)^2)

  }
  vals <- c(mean(MSE_mCV_ace[i,]), mean(MSE_mCV_h2s[i,]),
            mean(MSE_mCV_lac[i,]), mean(MSE_mCV_ace_h2s[i,]),
            mean(MSE_mCV_ace_lac[i,]), mean(MSE_mCV_h2s_lac[i,]),
            mean(MSE_mCV_3[i,]))
  if(i==1){
    models <- c("ace", "h2s", "lac", "ace_h2s", "ace_lac","h2s_lac", "3")
    plot(vals, xaxt="n", xlab="Models", ylab="MSE", col=i, type="b",
         pch=16)
    axis(1, at=1:7, labels=models)
  }else{
    points(vals, col=i, type="b", pch=16)
  }
}
```

-------------------------------------------------------
# activity: Cross Validation using mos

```{r}

mos.df<-read.table("data/mos_df.txt", header=TRUE, quote="\"")

library(boot)
glm.fit <- glm(Maxtemp~., data=mos.df)
cv.error <- cv.glm(mos.df, glm.fit, K=10)$delta[1]
cv.error
```

-------------------------------------------------------------------
# Cheddar dataset

```{r}
library(faraway)

data("cheddar")

res_lm_ace <- lm(taste ~ Acetic, data=cheddar)
res_lm_h2s <- lm(taste ~ H2S, data=cheddar)
res_lm_lac <- lm(taste ~ Lactic, data=cheddar)
res_lm2_ace_h2s <- lm(taste ~ Acetic + H2S, data=cheddar)
res_lm2_ace_lac <- lm(taste ~ Acetic + Lactic, data=cheddar)
res_lm2_h2s_lac <- lm(taste ~ H2S + Lactic, data=cheddar)
res_lm3 <- lm(taste ~ ., data=cheddar)

AIC <- AIC(res_lm_ace, res_lm_h2s, res_lm_lac, res_lm2_ace_h2s,
           res_lm2_ace_lac, res_lm2_h2s_lac, res_lm3)
BIC <- BIC(res_lm_ace, res_lm_h2s, res_lm_lac, res_lm2_ace_h2s,
           res_lm2_ace_lac, res_lm2_h2s_lac, res_lm3)

Adj.R2 <- vector(length=7)
Adj.R2[1] <- summary(res_lm_ace)$adj.r.squared
Adj.R2[2] <- summary(res_lm_h2s)$adj.r.squared
Adj.R2[3] <- summary(res_lm_lac)$adj.r.squared

Adj.R2[4] <- summary(res_lm2_ace_h2s)$adj.r.squared
Adj.R2[5] <- summary(res_lm2_ace_lac)$adj.r.squared
Adj.R2[6] <- summary(res_lm2_h2s_lac)$adj.r.squared
Adj.R2[7] <- summary(res_lm3)$adj.r.squared

models <- c("ace", "h2s", "lac", "ace_h2s", "ace_lac","h2s_lac", "3")

plot(AIC$AIC, xaxt="n", xlab="Models", ylab="AIC", col=2, type="b", pch=16)
axis(1, at=1:7, labels=models)

plot(BIC$BIC, xaxt="n", xlab="Models", ylab="BIC", col=2, type="b", pch=16)
axis(1, at=1:7, labels=models)

plot(Adj.R2, xaxt="n", xlab="Models", ylab="Adjusted R-squared", col=2,
     type="b", pch=16)
axis(1, at=1:7, labels=models)
```


--------------------------------------------------------------------------
# add one and drop one using Cheddar

drop 1
```{r}
library(faraway)
data("cheddar")

cheddar.lm2 <- lm(taste~H2S+Lactic, cheddar) # backward elimination
drop1(cheddar.lm2)
```

add 1 parameter to increase model complexity
```{r}
library(faraway)
data("cheddar")

cheddar.lm <- lm(taste~1,cheddar)
add1(cheddar.lm, scope=~Acetic+H2S+Lactic)
```

add a second parameter to increase model complexity
```{r}
library(faraway)
data("cheddar")

cheddar.lm2 <- lm(taste~H2S,cheddar)
add1(cheddar.lm2, scope=~Acetic+H2S+Lactic)
```

add a third additional parameter?
```{r}
library(faraway)
data("cheddar")

cheddar.lm3 <- lm(taste~H2S+Lactic,cheddar)
add1(cheddar.lm3, scope=~Acetic+H2S+Lactic)
```
no benefit from adding a 3rd additional parameter

----------------------------------------------------------------------
# MOS EXAMPLE

```{r}
mos.df <- read.table("data/mos_df.txt", header=TRUE, quote='"')
mos.lm <- lm(Maxtemp ~ ., mos.df)

drop1(mos.lm, scope~Modst+Modsp+Modthik)
```


```{r}
mos.df <- read.table("data/mos_df.txt", header=TRUE, quote='"')
mos2.lm <- lm(Maxtemp ~ Modsp + Modthik, mos.df)

drop1(mos2.lm, scope~Modsp+Modthik)
```


```{r}
mos.df <- read.table("data/mos_df.txt", header=TRUE, quote='"')
mos.lm <- lm(Maxtemp~1,mos.df)
add1(mos.lm,scope~Modst+Modsp+Modthik)
```


```{r}
mos.df <- read.table("data/mos_df.txt", header=TRUE, quote='"')
mos2.lm <- lm(Maxtemp~Modthik,mos.df)
add1(mos2.lm, scope~Modst+Modsp+Modthik)
```


```{r}
mos.df <- read.table("data/mos_df.txt", header=TRUE, quote='"')
mos3.lm <- lm(Maxtemp~Modthik+Modsp,mos.df)
add1(mos3.lm, scope~Modst+Modsp+Modthik)
```

---------------------------------------------------------------------
# Activity in R: Forward and Backward Stepwise Selection

Consider the `Hitters` dataset and use the `regsubsets()` function with the argument `method="forward"` or `method="backward"` to perform forward stepwise and backward stepwise selection. What is the best seven variable model?

# solution

```{r}
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
```

The best seven variable models identified by forward stepwise selection and backward stepwise selection are different and include the following variables:
```{r}
coef(regfit.fwd,7)
coef(regfit.bwd,7)
```

------------------------------------------------
#


```{r}
library(ISLR)
names(Hitters)
dim(Hitters)

library(leaps)
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)

regfit.bwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)

coef(regfit.fwd,7)

coef(regfit.bwd,7)

```

--------------------------------------------------------------------
# example: hospital manpower data
```{r}
require(bestglm)
data(manpower)
```


#
```{r}
require(bestglm)
require(glmnet)

x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-6, 6, 0.2))
fit <- glmnet(x, y, alpha=0, lambda=lambda)
plot(fit,xvar="lambda",label=T)
```


```{r}
require(bestglm)
require(glmnet)

x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-6, 6, 0.2))

set.seed(1) # For reproducibility
cv.fit <- cv.glmnet(x, y, alpha=0, nfolds=5, lambda=lambda)
plot(cv.fit)
```


```{r}
require(bestglm)
require(glmnet)

x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-6, 6, 0.2))

set.seed(1) # For reproducibility
cv.fit <- cv.glmnet(x, y, alpha=0, nfolds=5, lambda=lambda)
lambda <- cv.fit$lambda.min
lambda
```


```{r}
require(bestglm)
require(glmnet)

x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-6, 6, 0.2))

set.seed(1) # For reproducibility
cv.fit <- cv.glmnet(x, y, alpha=0, nfolds=5, lambda=lambda)
lambda <- cv.fit$lambda.1se
lambda
```

#

```{r}
require(bestglm)
require(glmnet)

x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-6, 6, 0.2))

fit <- glmnet(x, y, alpha=0, lambda=lambda)

set.seed(1) # For reproducibility
cv.fit <- cv.glmnet(x, y, alpha=0, nfolds=5, lambda=lambda)

fit$beta[,cv.fit$index[,1]]
fit$a0[cv.fit$index[,1]]
```

--------------------------------------------------------------------
# Activity in R: Ridge Regression 1

The data set `hald` was first printed in the article by Woods, Steinour and Starke (1932), "Effect of Composition of Portland Cement on Heat Evolved during Hardening," Industrial and Engineering Chemistry, 24, pp. 1207-14.

With these data we are interested in predicting the response yy (heat
evolved in calories per gram of cement) in terms of the predictors `x1`, `x2`, `x3` and `x4`.

Construct pairwise scatter plots of the predictor variables. Do you think multicollinearity is present in these data?

Find the ridge estimators for the coefficients of the standardised predictors when `λ=0.02`, using ordinary least squares regression on an augmented dataset. We augment the standardized matrix `X` with `p` additional rows λIλI, and augment `y` with `p` zeros. By introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients toward zero.

Compare the parameter estimates with the estimates obtained from `glmnet()` function. Are they approximately the same? Note that to compare `glmnet()` to other methods authors of `glmnet()` suggest to standardise the response variable, too.

```{r}
hald<-read.table("data/hald.txt", header=T)

print(dim(hald))

pairs(hald)

cor(hald)

y<-(hald$y-mean(hald$y))/sqrt(var(hald$y))
X<-as.matrix(hald[,-1])
pmeans<-apply(X,2,mean)
n<-length(y)
pscale<-apply(X,2,var)
pscale<-sqrt(pscale)
matrix(rep(pmeans,times=n),nrow=n,byrow=T)

X1<-X-matrix(rep(pmeans,times=n),nrow=n,byrow=T)
X2<-X1%*%diag(1/pscale)
lambda<-0.02
Xstar<-rbind(X2,sqrt(lambda)*diag(4))
ystar<-c(y,rep(0,times=4))
Xstar
ystar

haldridge.lm<-lm(ystar ~ Xstar-1)
haldridge.lm$coef

mean(y)

library(glmnet)
fit<-glmnet(X2,y, alpha=0,lambda=0.02,standardize=FALSE)

bet.s<-fit$beta
a0.s<-fit$a0

bet.s
a0.s
```

-----------------------------------------------------

# Activity Ridge Regression 2

In this exercise we will analyse the properties of the ridge regression through an application to the `credit` dataset available in the `ISLR` package in R.

Load the dataset. Define the response vector by y and the design matrix X
using the function `model.matrix()`. The function `model.matrix()` prepares the predictors to be included in the ridge regression via the `glmnet()` in the correct format (numerical or quantitative outputs only).

1. Define a grid for the tuning parameter `λ`, ranging from `λ=10−2` to
   `λ=105` in decreasing order. Perform a ridge regression over the defined
   grid of λ values. This covers a range of scenarios from the null model
   containing only the intercept, to the least square fit. Use the function
   `plot(,xvar="lambda")` to plot the ridge regression coefficients for the
   predictor IncomeIncome as a function of `λ`.

2. When λ is small, ridge regression gives similar answers to ordinary
   regression. Check this assertion by comparing the estimates for the
   ordinary regression and ridge regression with the smallest `λ` considered.

3. When `λ` is large, ridge regression shrinks the parameter estimates when
   compared to the least squares estimates. Check this assertion by comparing
   the estimates for the ordinary regression and ridge regression with the
   largest `λ` considered.

4. Split the data equally into training and test sets using `set.seed(1)`.
   When `λ` is small, we get only small improvement in the test error over
   linear regression, while when λ is large we see a definite improvement, λ
   cannot be too large though. Check this assertions by computing the test
   MSE for the ordinary regression and the ridge regression penalty parameter
   fixed to `λ=0.01,7` and 20

5. In general, rather than arbitrarily choosing λ=7, it would be better to
   use cross-validation to choose the tuning parameter λ. We can do this
   using the built-in cross validation function `cv.glmnet()`. By default the
   function performs 10-fold cross-validation, though it can be changed using
   the argument `folds`. Use 5-fold cross validation to select the optimal
   tuning parameter `λ` and plot the output (MSE as function of log(λ)). What
   is the value of the tuning parameter than results in the smallest
   cross-validation error and what is the associated test MSE value? For
   reproducibility use `set.seed(2)`.

6. From the plot in the previous question, the λmin⁡λmin​ seems to be
   suspiciously close to the boundary of the search grid. We therefore decide
   to re-run the cross-validation algorithm using the search grid that we
   initially defined. Do you observe any changes? For reproducibility use
   `set.seed(2)`.


```{r}
library(ISLR)

# Question 1

data("Credit")
attach(Credit)

Credit <- Credit[,-1]

y <- Credit$Balance
X <- model.matrix(Balance~., data=Credit)[,-1]

# Question 2

library(glmnet)

grid <- 10^seq(5,-2,length=100)
ridge.mod <- glmnet(X,y, alpha=0, lambda=grid)

plot(seq(5,-2,length=100),coef(ridge.mod)[2,], type="l",
     ylab="Coefficient", xlab=expression(log(lambda)/log(10)))

# Question 3

lm.mod <- lm(y~X)
cbind(lm.mod$coefficients,coef(ridge.mod)[,100])

# Question 4

cbind(lm.mod$coefficients,coef(ridge.mod)[,1])

# Question 5

set.seed(1)
train <- sample(1:nrow(X),nrow(X)/2)
test <- -train
linear.mod <- lm(y[train]~X[train,])
linear.pred <-  coef(linear.mod)[1]+X[test,] %*% coef(linear.mod)[-1]
mean((linear.pred-y[test])^2)

ridge.mod <- glmnet(X[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=tail(grid,1), newx=X[test,])
mean((ridge.pred-y[test])^2)

ridge.pred <- predict(ridge.mod, s=7, newx=X[test,])
mean((ridge.pred-y[test])^2)

ridge.pred <- predict(ridge.mod, s=20, newx=X[test,])
mean((ridge.pred-y[test])^2)

# Question 6

set.seed(2)
cv.out <- cv.glmnet(X[train,], y[train], alpha=0, nfolds=5)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

ridge.pred <- predict(ridge.mod, s=bestlam, newx=X[test,])
mean((ridge.pred-y[test])^2)

# Question 7

set.seed(2)
cv.out <- cv.glmnet(X[train,], y[train], alpha=0, lambda=grid, nfolds=5)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

ridge.pred <- predict(ridge.mod, s=bestlam, newx=X[test,])
mean((ridge.pred-y[test])^2)
```

#

```{r}
require(bestglm)
require(glmnet)

data(manpower)
x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-8.1, 1, 0.1))
fit2 <- glmnet(x, y, alpha=1, lambda=lambda)

set.seed(1) # For reproducibility
cv.fit2 <- cv.glmnet(x, y, alpha=1, nfolds=5, lambda=lambda)
plot(fit2,xvar="lambda",label=T)
```



```{r}
require(bestglm)
require(glmnet)

data(manpower)
x <- as.matrix(manpower[, 1:4])
y <- log(manpower$Hours)
lambda <- exp(seq(-8.1, 1, 0.1))
fit2 <- glmnet(x, y, alpha=1, lambda=lambda)

set.seed(1) # For reproducibility
cv.fit2 <- cv.glmnet(x, y, alpha=1, nfolds=5, lambda=lambda)
plot(cv.fit2)
bestlam <- cv.fit2$lambda.min
bestlam
```

# Hitters dataset

```{r}
library(ISLR)
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
```


```{r}
require(bestglm)
require(glmnet)
library(ISLR)
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
lambda <- exp(seq(-8.1, 1, 0.1))

set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
bestlam <- cv.out$lambda.min
bestlam

## From R3.6.0 or later
## [1] 326.0828
## Before R 3.6.0
## [1] 211.7416

```

#

```{r}
require(bestglm)
require(glmnet)
library(ISLR)
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
lambda <- exp(seq(-8.1, 1, 0.1))

set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
bestlam <- cv.out$lambda.min

ridge.mod <- glmnet(x[train,], y[train], alpha=0)
ridge.pred <- predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)
```


```{r}
require(bestglm)
require(glmnet)
library(ISLR)
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
lambda <- exp(seq(-8.1, 1, 0.1))

set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
bestlam <- cv.out$lambda.min

out <- glmnet(x, y, alpha=0)
predict(out, type="coefficients",s=bestlam)[1:20,]
```

```{r}
require(bestglm)
require(glmnet)
library(ISLR)
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
lambda <- exp(seq(-8.1, 1, 0.1))

set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min
bestlam

## [1] 9.286955
## [1] 16.78016  before R 3.6.0

lasso.mod <- glmnet(x[train,], y[train], alpha=1)
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)

## [1] 143668.8
## [1] 100838.2 before R 3.6.0
```


```{r}
require(bestglm)
require(glmnet)
library(ISLR)
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
lambda <- exp(seq(-8.1, 1, 0.1))

set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min

out <- glmnet(x, y, alpha=1)
predict(out, type="coefficients",s=bestlam)[1:20,]
```

# model selection
The `BigMac2003` data set can be found in the package `alr4` in R (source: Cook and Weisberg, “Applied Regression Including Computing and Graphics,” Wiley, 1999)

The Big Mac hamburger is a simple commodity that can be used to study the inefficiency in currency exchange, see an article in the Economist.

(a) Confirm that a log-transformation is appropriate for all variables which are measured in units of currency (minutes of labor).

(b) Assume the log-price of a BigMac as the response and carry out a best-subset linear regression analysis.

Compute the AIC, BIC, five- and tenfold cross-validation of prediction error for the best model and the full model. Discuss the results. (Hint: you may use the package "bestglm".)

(c) Compare the diagnostic plots for the chosen model and the full model, e.g. by overlaying each plot. Which cities are most influential for the fits? Are there any outliers?

(d) Assuming that it is unknown, give a confidence interval and a prediction interval for the price of a BigMac in Sydney. Which model do you suggest using for the prediction?

```{r}
require(alr4)

data(BigMac2003)
head(BigMac2003)

# (a)
pairs(BigMac2003)

data <- BigMac2003
data$BigMac <- log(data$BigMac)
data$Bread <- log(data$Bread)
data$Rice <- log(data$Rice)
data$Bus <- log(data$Bus)
data$Apt <- log(data$Apt)
data$TeachGI <- log(data$TeachGI)
data$TeachNI <- log(data$TeachNI)
pairs(data)

# (b)
require(bestglm)

Xy <- data[, c(2:10, 1)]
bestfits <- bestglm(Xy, IC = "AIC")
bestfits$BestModels

fit <- glm(BigMac~Bread+Rice+FoodIndex+TeachGI, data=data)
fit2 <- glm(BigMac~., data=data)

AIC(fit)
BIC(fit)
AIC(fit2)
BIC(fit2)

require(boot)
set.seed(1)
cv <- cv.glm(data = data, glmfit = fit, K = 5)
set.seed(1)
cv2 <- cv.glm(data = data, glmfit = fit, K = 10)

cv$delta
cv2$delta

set.seed(1)
cv <- cv.glm(data=data, glmfit=fit2, K=5)
set.seed(1)
cv2 <- cv.glm(data=data, glmfit=fit2, K=10)

cv$delta
cv2$delta

# (c)
par(mfrow=c(1,2))
plot(fit, which=1, main="best model")
plot(fit2,which=1, main="full model")

par(mfrow=c(1,2))
plot(fit, which=2, main="best model")
plot(fit2,which=2, main="full model")

par(mfrow=c(1,2))
plot(fit, which=3, main="best model")
plot(fit2,which=3, main="full model")

par(mfrow=c(1,2))
plot(fit, which=4, main="best model")
plot(fit2,which=4, main="full model")

par(mfrow=c(1,2))
plot(fit, which=5, main="best model")
plot(fit2,which=5, main="full model")

# (d)
newdata <- data.frame(data[61, 2:10])
fit <- lm(BigMac~Bread+Rice+FoodIndex+TeachGI, data=data)
fit2 <- lm(BigMac~., data=data)
pi1<-predict(fit,newdata,interval="predict")
pi1

predict(fit,newdata,interval="confidence")

pi2<-predict(fit2,newdata,interval="predict")
pi2

predict(fit2,newdata,interval="confidence")

pi1[,3] - pi1[,2]

pi2[,3] - pi2[,2]
```

# Solution

# a)
Confirm that a log-transformation is appropriate for all variables which are measured in units of currency.

```{r}
require(alr4)
data(BigMac2003)
head(BigMac2003)

# (a)
pairs(BigMac2003)

data <- BigMac2003
data$BigMac <- log(data$BigMac)
data$Bread <- log(data$Bread)
data$Rice <- log(data$Rice)
data$Bus <- log(data$Bus)
data$Apt <- log(data$Apt)
data$TeachGI <- log(data$TeachGI)
data$TeachNI <- log(data$TeachNI)
pairs(data)
```

After the log-transformation of variables measured in units of currency, the pairs plot looks more linear.

# b)
Assume the log-price of a BigMac as the response, and carry out a best-subset linear regression analysis. Compute the AIC, BIC, five- and tenfold cross-validation of prediction error for the best model and the full model. Discuss the results. (Hint: you may use the package bestglm.)

bestglm() is the best subset selection using 'leaps' algorithm (Furnival and Wilson, 1974) or complete enumeration (Morgan and Tatar, 1972). Complete enumeration is used for the non-Gaussian and for the case where the input matrix contains factor variables with more than 2 levels. The best fit may be found using the information criterion IC: AIC, BIC, EBIC, or BICq. Alternatively, with IC=`CV' various types of cross-validation may be used.

```{r}
require(alr4)

data(BigMac2003)
head(BigMac2003)

# (a)
data <- BigMac2003
data$BigMac <- log(data$BigMac)
data$Bread <- log(data$Bread)
data$Rice <- log(data$Rice)
data$Bus <- log(data$Bus)
data$Apt <- log(data$Apt)
data$TeachGI <- log(data$TeachGI)
data$TeachNI <- log(data$TeachNI)

# (b)
require(bestglm)

Xy <- data[, c(2:10, 1)]
bestfits <- bestglm(Xy, IC = "AIC")
bestfits$BestModels

fit <- glm(BigMac~Bread+Rice+FoodIndex+TeachGI, data=data)
fit2 <- glm(BigMac~., data=data)

AIC(fit)
BIC(fit)
AIC(fit2)
BIC(fit2)

require(boot)
set.seed(1)
cv <- cv.glm(data = data, glmfit = fit, K = 5)
set.seed(1)
cv2 <- cv.glm(data = data, glmfit = fit, K = 10)

cv$delta
cv2$delta

set.seed(1)
cv <- cv.glm(data=data, glmfit=fit2, K=5)
set.seed(1)
cv2 <- cv.glm(data=data, glmfit=fit2, K=10)

cv$delta
cv2$delta
```

The bestglm() function identified 5 best models when the AIC was used as the information criterion.

The best model with four predictors has AIC and BIC

```

```








