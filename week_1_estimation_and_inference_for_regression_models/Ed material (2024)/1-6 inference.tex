%! Author = noone
%! Date = 1/21/24

% Preamble
\documentclass[11pt]{article}

% Remove paragraph indentation
\setlength{\parindent}{0pt}

% Increase space between paragraphs
\setlength{\parskip}{1em}  % Adjust the value as needed

% Discourage hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Packages
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}

% Document
\begin{document}

\section{Inference}

The two main tools in statistics to make conclusions are

\begin{itemize}
    \item \textbf{Confidence intervals:} the width of a confidence interval provides a measure of the precision of the point estimates.
    \item \textbf{Hypothesis testing:} it compares how well two related models fit the data. The logic can be summarised as follows:
    \begin{enumerate}
        \item Specify a model corresponding to \(H_0\) and a more general model corresponding \(H_1\).
        \item Fit model \(M_0\) and compute measure of goodness of fit; repeat for \(M_1\) to obtain \(G_0\) and \(G_1\).
        \item Calculate the improvement in fit.
        \item Test the null hypothesis \(G_0 = G_1\).
        \item If \(H_0\) is not rejected then \(M_0\) is the preferred model.
    \end{enumerate}
\end{itemize}

For confidence intervals and hypothesis testing, sampling distributions are required.

\begin{itemize}
    \item For \textbf{normally distributed random variables}, the sampling distribution can be determined exactly.
    \item For \textbf{other distributions}, we need to rely on large-sample asymptotic results based on the Central Limit Theorem (CLT).
\end{itemize}

The basic idea is that under appropriate conditions (i.i.d and $S$ being a sum), the statistic of interest \(S\) is

\[ \dfrac{S - \mathbb{E}(S)}{\sqrt{\mathbb{V}\text{ar}(S)}} \dot\sim \mathcal{N}(0, 1)\]

or equivalently

\[ \frac{[S - \mathbb{E}(S)]^2}{\mathbb{V}\text{ar}(S)} \dot\sim \chi^2_1\]

and in the case of \(p\)-multivariate statistics

\[[S - \mathbb{E}(S)]^\top V^{-1} [S - \mathbb{E}(S)] \dot\sim \chi^2_p\]

\section{Sampling distribution for score statistics}

Suppose \(Y_1, \ldots, Y_N\) are independent random variables from a distribution which belongs to the exponential family with parameter \(\theta = (\theta_1, \ldots, \theta_p)\).

The \textbf{score statistics} are such that

\[\mathbb{E}[U_j] = 0 \text{ for all } j = 1, \ldots, p\]

The \textbf{variance-covariance matrix of the score statistics} is the information matrix \(\mathcal{I}\) with elements

\[\mathcal{I}_{jk} = \mathbb{E}[U_j U_k]\]

If \(p = 1\), the score statistic has the asymptotic sampling distribution

\[\dfrac{U}{\sqrt{\mathcal{I}}} \dot\sim \mathcal{N}(0, 1)\]

where the symbol \(\dot\sim\) means \"approximately distributed as\". Equivalently we can write

\[\dfrac{U^2}{\mathcal{I} \dot\sim} \chi^2_1\]

If \(p > 1\),

\[U \dot\sim} \mathcal{MVN}(0, \mathcal{I})\]

or equivalently

\[U^\top \mathcal{I}^{-1} U \dot\sim} \chi^2_p\]

\section{Example: Binomial distribution}

If \(Y \sim \text{Bin}(n, p)\), the log-likelihood function is

\[\ell(p; y) = y \log p + (n - y) \log(1 - p) + \log \binom{n}{y}\]

and the score statistic is

\[U = \frac{d\ell}{dp} = \frac{Y}{p} - \frac{n - Y}{1 - p} = \frac{Y - np}{p(1 - p)}\]

Since \(E(Y) = np \), \( E(U) = 0\) as expected.

Since \( \text{Var}(Y) = np(1 - p) \), the information is

\[ I = \text{Var}(U) = \frac{1}{p^2(1 - p)^2} \text{Var}(Y) = \frac{n}{p(1 - p)} \]

and hence

\[\frac{U}{\sqrt{I}} = \frac{Y - np}{\sqrt{np(1 - p)}} \stackrel{\cdot}{\sim} \mathcal{N}(0, 1)\]

This is known as the normal approximation to the binomial distribution.

\section{Taylor Approximation}

To obtain the asymptotic sampling distributions for various statistics, it is useful to use Taylor approximations for generic functions in a neighborhood of \( t \).

\[f(x) = f(t) + (x - t)\left[ \frac{df}{dx} \right]_{x=t} + \frac{1}{2}(x - t)^2 \left[ \frac{d^2f}{dx^2} \right]_{x=t} + \ldots\]

For a log-likelihood, the first three terms are

\[\ell(\theta) = \ell(\hat{\theta}) + (\theta - \hat{\theta})U(\hat{\theta}) - \frac{1}{2}(\theta - \hat{\theta})^2I(\hat{\theta})\]

where \(\hat{\theta}\) is the MLE of \(\theta\).

For a \(p\)-dimensional vector \(\theta\),

\[ \ell(\theta) = \ell(\hat{\theta}) + (\theta - \hat{\theta})^\top u(\hat{\theta}) - \frac{1}{2}(\theta - \hat{\theta})^\top I(\hat{\theta})(\theta - \hat{\theta})\]

Similarly, for the score function of a one-dimensional parameter \(\theta\), the first two terms of the Taylor approximation are

\[ U(\theta) \approx U(\hat{\theta}) + (\theta - \hat{\theta})U'(\hat{\theta}) = U(\hat{\theta}) - (\theta - \hat{\theta})I(\hat{\theta})\]

and the score function of a \( p \)-dimensional parameter \(\theta\) becomes

\[ u(\theta) \approx u(\hat{\theta}) - I(\hat{\theta})(\theta - \hat{\theta})\]

\section{Sampling distribution for score statistics}

Suppose \[Y_1, \ldots, Y_N \] are independent random variables from a distribution which belongs to the exponential family with parameter \( \theta = (\theta_1, \ldots, \theta_p) \).

The score statistics are such that

\[\mathbb{E}[U_j] = 0 \text{ for all } j = 1, \ldots, p\]

The variance-covariance matrix of the score statistics is the information matrix \(\mathcal{I}\) with elements

\[\mathcal{I}_{jk} = \mathbb{E}[U_j U_k]\]

If \(p = 1\), the score statistic has the asymptotic sampling distribution

\[\frac{U}{\sqrt{\mathcal{I}}} \dot\sim \mathcal{N}(0, 1)\]

where the symbol \(\dot\sim\) means "approximately distributed as". Equivalently we can write

\[\frac{U^2}{\mathcal{I}} \dot\sim \chi^2_1\]

If \(p > 1\),

\[U \dot\sim \mathcal{MVN}(0, I)\]

or equivalently

\[U^\top I^{-1} U \dot\sim \chi^2_p\]

\section{Sampling distribution of the MLE}

Let's define the MLE (Maximum Likelihood Estimator) as \(\hat{\theta}\).

By definition, the MLE is the estimator which maximizes \(\ell(\hat{\theta})\), i.e., \(u(\hat{\theta}) = 0\).

\begin{align} \nonumber
    u(\theta) &\approx -I(\hat{\theta})(\theta - \hat{\theta}) \nonumber \\
    -\mathcal{I}(\hat{\theta})^{-1}u(\theta) &\approx (\theta - \hat{\theta}) \nonumber \\
\end{align} \nonumber

Properties:

\begin{itemize}
    \item \textbf{Consistency:} Since \( \mathbb{E}(u) = 0 \), \nonumber
\end{itemize}

\begin{align}
    \mathbb{E}(\hat{\theta} - \theta) &= 0 \implies \mathbb{E}(\hat{\theta}) = \theta \nonumber \\
    \mathbb{E}(u) &= \mathbb{E}(\mathcal{I}(\hat{\theta})(\hat{\theta} - \theta)) \nonumber \\
    0 &= \mathcal{I}(\hat{\theta}) \mathbb{E}(\hat{\theta} - \theta) \nonumber \\
    \mathcal{I}(\hat{\theta})^{-1}0 &= \mathbb{E}(\hat{\theta} - \theta) \nonumber \\
\end{align} \nonumber

\begin{itemize}
    \item \textbf{Variance-covariance matrix:}
\end{itemize}

\begin{align}
    \mathbb{E}[(\hat{\theta} - \theta)(\hat{\theta} - \theta)^\top] &= \mathbb{E}[\mathcal{I}^{-1}uu^\top \mathcal{I}^{-1}] \nonumber \\
    &= \mathcal{I}^{-1}E[uu^\top] \mathcal{I}^{-1} = \mathcal{I}^{-1} \nonumber \\
\end{align}

\begin{itemize}
    \item \textbf{Asymptotic sampling distribution:}
\end{itemize}

\begin{align}
    (\hat{\theta} - \theta)^\top \mathcal{I}(\hat{\theta})(\hat{\theta} - \theta) \dot\sim \chi^2(p) \nonumber
\end{align}

Remarks:
\begin{itemize}
    \item \((\hat{\theta} - \theta)^\top \mathcal{I}(\hat{\theta})(\hat{\theta} - \theta)\) is also known as Wald statistics.
    \item For one-dimensional parameter, you can write \(\hat{\theta} \dot\sim \mathcal{N}(\theta, \mathcal{I}^{-1})\).
    \item If the response variable is normally distributed, the results are exact; for other GLM (Generalized Linear Models), the results are asymptotic.
\end{itemize}

\end{document}