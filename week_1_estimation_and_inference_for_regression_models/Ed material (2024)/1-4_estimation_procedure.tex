%! Author = noone
%! Date = 1/21/24

% Preamble
\documentclass[11pt]{article}

% Remove paragraph indentation
\setlength{\parindent}{0pt}

% Increase space between paragraphs
\setlength{\parskip}{1em}  % Adjust the value as needed

% Discourage hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Packages
\usepackage{amsmath}

\begin{document}

\section{Estimation procedure}

\subsection{Introduction}

This section is about obtaining
\begin{itemize}
    \item Point estimates
    \item Interval estimates
\end{itemize}

In many settings analytical solutions are not available so we will need \textbf{numerical methods} (e.g. Newton-Raphson algorithm).

\textbf{Recommended reading:}

A. J. Dobson \& A. G. Barnett (2018) An introduction to generalised linear models Chapman and Hall. Chapters 345.

\subsection{Estimation procedure}

Let's consider a dataset including lifetimes (times to failure in hours) of Kevlar epoxy strand pressure vessels at 70\% stress level. (see Andrews and Herzberg 1985.)

\begin{verbatim}
pressure <- c(1051, 1337, 1389, 1921, 1942, 2322, 3629, 4006, 4012,
4063, 4921, 5445, 5620, 5817, 5905, 5956, 6068, 6121,
6473, 7501, 7886, 8108, 8546, 8666, 8831, 9106, 9711,
9806, 10205, 10396, 10861, 11026, 11214, 11362, 11604,
11608, 11745, 11762, 11895, 12044, 13520, 13670, 14110,
14496, 15395, 16179, 17092, 17568, 17568)
\end{verbatim}

\subsection{The Weibull distribution}

A commonly used model for times to failure (or survival times) is the Weibull distribution:

\begin{equation}
    f(y \mid \lambda, \theta) = \frac{\lambda y^{\lambda - 1}}{\theta^\lambda} \exp\left[- \left(\frac{y}{\theta}\right)^\lambda\right] \quad (1.4.1)
\end{equation}

where
\begin{itemize}
    \item \( y > 0 \) is the time to failure
    \item \( \lambda \) is the shape parameter
    \item \( \theta \) is the scale parameter
\end{itemize}

\subsection{Probability Density Function of the Weibull Distribution}

The pdf of the Weibull distribution (1.4.1) can be rewritten as:

\begin{equation}
    f(y \mid \lambda, \theta) = \exp \left( \log\left[\frac{\lambda}{\theta}\right] + (\lambda - 1) \log y - \lambda \log \theta - \left(\frac{y}{\theta}\right)^\lambda \right)
\end{equation}

\subsubsection{Likelihood Function for the Weibull Distribution}

Let \( Y_1,\dots,Y_N \) be the data and \( N = 49 \). Let's assume that the pressure vessels are independent and identically distributed; then the \textbf{likelihood} is

\begin{equation}
    f(y_1,...,y_N|\lambda, \theta) = \prod_{i=1}^{N} \dfrac{\lambda y_{i}^{\lambda-1}}{\theta^{\lambda}} \exp \left[ - \left( \dfrac{y_i}{\theta} \right)^{\lambda} \right]
\end{equation}

and the \textbf{log-likelihood}

\begin{equation}
    l(\theta, \lambda; y_1,...,y_N) = \sum_{i=1}^{N} \left[ (\lambda - 1) \log y_i + \log \lambda - \lambda \log \theta - \left( \frac{y_i}{\theta} \right)^\lambda \right]
\end{equation}

\subsection{Estimation of the Weibull Parameters}

Let's focus on \(\theta\) and consider for the moment \(\lambda\) as known.

To obtain the maximum likelihood estimator we need to derive the derivative of the log-likelihood with respect to \(\theta\). This is also known as the \textbf{score function}:

\begin{equation}
    \frac{d\ell}{d\theta} = U = \sum_{i=1}^{N} \left[ -\frac{\lambda}{\theta} + \lambda \frac{\lambda y_i^{\lambda}}{\theta^{\lambda+1}} \right]
\end{equation}

and the \textbf{maximum likelihood estimator} is obtained by setting \(U = 0\).

Let's suppose \(\lambda = 2\). In this simple case the MLE is:

\begin{equation}
    \sum_{i=1}^{N} \left[ -\frac{\lambda}{\theta} + \lambda \frac{y_i^\lambda}{\theta^{\lambda+1}} \right] -\frac{N\lambda}{\theta} + \lambda \frac{1}{\theta^{\lambda+1}} \sum_{i=1}^{N} y_i^\lambda = 0
\end{equation}

Consequently, we have \(\lambda \sum_{i=1}^{N} y_i^\lambda = N\lambda\theta^\lambda\) which implies

\begin{equation}
    \hat{\theta} = \left( \frac{\sum_{i=1}^{N} y_i^\lambda}{N} \right)^{\frac{1}{\lambda}} = 9,892.177
\end{equation}

\subsection{Newton-Raphson Algorithm in 1D}

Suppose \(f: \mathbb{R} \rightarrow \mathbb{R} \) is differentiable and attains the value 0 at \( x_0 \).
The \textbb{Newton-Raphson algorithm} finds \( x_0 \) iteratively. The slope of \( f \) at a value \( x_{(m-1)} \) is given by:

\begin{equation}
    \left[ \dfrac{df}{dx}\right]_{x=x^{m-1}} = f'(x_{(m-1)}) \approx \dfrac{f(x^{(m)}) - f(x^{(m-1)})}{x^{(m)} - x^{(m-1)}}
\end{equation}

if the distance \( x^{(m)} - x^{(m-1)} \) is small (mean value theorem). If \( x^{(m)} = x_0 \) is the required solution so that \( f(x^m) = 0 \) holds, then the above can be re-arranged to

\begin{equation}
    x^{(m)} = x^{(m-1)} - \dfrac{f(x^{(m-1)})}{f'(x^{(m-1)})}
\end{equation}

Iterating this algorithm creates a sequence \(x^{(m)}\) which approaches \(x_0\).

See here for a nice visualization on Wikipedia by Ralf Pfeifer.

For the Weibull distribution with \(\lambda = 2\), we have

\begin{equation}
    U = -\dfrac{2N}{\theta} + \dfrac{2 \sum_{i=1}^{N} y_i^2}{\theta^3}
\end{equation}

and the derivative

\begin{equation}
    \dfrac{dU}{d\theta} = U' = \dfrac{2N}{\theta^2} - \frac{2 \times 3 \times \sum_{i=1}^{N} y_i^2}{\theta^4}
\end{equation}

\subsection{Newton-Raphson Algorithm in pD}

Assume here that \(\theta\) denotes a vector of parameters of length \(p\).

Define \(U_j := \dfrac{\partial \ell}{\partial \theta_j}\) as the score with respect to \(\theta_j\) and write \(u := (U_1, \ldots, U_p) \). Similarly to \(x^{(m)}\), we now construct a vector sequence $\theta^m$ converging to \(\hat{\theta}\) where \(u(\hat{\theta})=0\). We write \(u^{(m)} := u(\theta^{(m)})\). Again, by the mean value theorem we have

\begin{equation}
    u^{(m)} - u^{(m-1)} \approx H_{\ell}^{(m-1)} (\theta^{(m)} - \theta^{(m-1)})
\end{equation}

where the Hessian $H_{\ell}$ is the \( p \times p \)-matrix with entry \( \dfrac{\partial^2 \ell}{\partial \theta_j \partial\theta_k}\) at position \( (j, k) \).

Aiming for \(u^{(m)} = 0\) as before, we arrive at a Newton-Raphson step:

\begin{equation}
    \theta^{(m)} = \theta^{(m-1)} - (H_{\ell}^{(m-1)})^{-1} u^{(m-1)}
\end{equation}

\textbf{Method of Scoring:}

It is possible to calculate \(H_{\ell}^{(m)}\) at each iteration step $\theta^{(m)}$ via the chain rule. However, it has been shown empirically that no significant loss of accuracy is suffered when \(H_{\ell}^{(m)}\) is replaced by minus the information matrix \(-\mathcal{I} := \mathbb{E}[U']\).

That is, instead of (1.4.6), one iterates:

\begin{equation}
    \theta^{(m)} = \theta^{(m-1)} + (\mathcal{I}^{(m-1)})^{-1} u^{(m-1)}
\end{equation}

This is called the \textbf{method of scoring}.

\subsection{Likelihood Maximisation}

As we have seen, the MLE is \(\hat{\theta} = 9892.17\). The curvature of the function in the vicinity of the \(\ell\) maximum gives information about how reliable the MLE is.

The curvature of \(\ell\) is defined by the rate of change of \(U\), i.e., \(U'\) or \(\mathbb{E}[U']\):

\begin{itemize}
    \item If \(U'\) is small, then \(\ell\) is flat and \(U\) is small for a wide interval of values for \(\theta\).
    \item If \(U'\) is large, then \(\ell\) is concentrated around \(\hat{\theta}\).
\end{itemize}

We will see that the variance of \(\hat{\theta}\) is inversely related to \(\mathcal{I} = \mathbb{E}[-U']\):

\begin{equation}
    \text{s.e.}(\hat{\theta}) = \frac{1}{\sqrt{\mathcal{I}}}
\end{equation}

\end{document}
