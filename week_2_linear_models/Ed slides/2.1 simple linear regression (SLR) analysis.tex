%! Author = noone
%! Date = 1/23/24

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Remove paragraph indentation
\setlength{\parindent}{0pt}

% Increase space between paragraphs
\setlength{\parskip}{1em}  % Adjust the value as needed

% Discourage hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Document
\begin{document}

\section{Simple Linear Regression (SLR) Analysis}

\subsection{Introduction}

Simple linear regression (SLR) is a method to explain the relationship between two quantitative variables using a straight line. One variable is a response variable ($Y$) and the other one is a predictor variable ($X$).

In this section, we will work with the Boston dataset (a famous housing dataset available in R) and show how to perform a simple linear regression analysis on this particular dataset. Each step of the analysis will be preceded with some theoretical background. We represent data as pairs of \( n \) observations:

\begin{equation}
    \[(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\]
\end{equation}

Each pair consists of measurements of variables $X$ and $Y$.

We will describe in detail how to calculate the intercept and slope estimates in the simple linear regression problem. Additionally, we will discuss how to assess the accuracy of the parameter estimates and how to assess the accuracy of the SLR model itself. We will also identify some potential problems arising in linear regression in general.

The purpose of this section is not only to introduce simple linear regression but also to identify the main steps of statistical analysis which can later be applied to more complex statistical models.

\subsection{Example: Simple Linear Regression (SLR) Analysis}

The `MASS` R library contains the `Boston` dataset which records `medv` (median house value) for 506 neighbourhoods around Boston. Fit a simple linear regression model using medv as your response variable and lstat (percent of households with low socioeconomic status) as your predictor variable.

We will perform the simple \textbf{linear regression analysis} in the following \textbf{steps}:

\begin{enumerate}
    \item Inspect, summarize, and visualize your dataset.
    \item Produce a scatter diagram of the response variable versus the explanatory variable. What is the relationship between these two variables?
    \item Fit the SLR model using the \texttt{lm()} function in R. Write down the resulting regression equation. What does this equation tell you?
    \item Assess the accuracy of the coefficient estimates using the R output.
    \item Assess the accuracy of the SLR model.
    \item Identify any potential problems in your analysis by using diagnostic plots.
    \item Use the regression equation to predict the value of medv for lstat values of 5, 10, and 15.
\end{enumerate}

Below the text in blue refers directly to our example while the text in black explains the theoretical background or presents other information needed for the analysis.

\subsection{Step 1: Inspecting and Summarising the Data}

As the first step, we load the \texttt{MASS} library in R and display names of all the variables in the Boston dataset using the \texttt{names()} function. Type \texttt{?Boston} to see the description of the Boston dataset in the help window.

\begin{verbatim}
> library(MASS)
> ?Boston
(press q to close it)
> names(Boston)
[1] "crim" "zn" "indus" "chas" "nox" "rm" "age"
[8] "dis" "rad" "tax" "ptratio" "black" "lstat" "medv"
\end{verbatim}

The \texttt{dim()} function tells us that the dataset has 506 rows or observations and 14 columns or variables.

\begin{verbatim}
> dim(Boston)
[1] 506 14
\end{verbatim}

There are several options in R to view the data. The \texttt{head()} function returns the first \( n \) rows (\( n = 6 \) by default) of a dataset. The \texttt{fix()} function can be used to view the data in a spreadsheet-like window. Note that the window must be closed before further R commands can be entered.

\begin{verbatim}
> head(Boston)
\end{verbatim}

\[ \text{[Data table of the first few rows of the Boston dataset is displayed here]} \]

The \texttt{summary()} function produces a numerical summary for each variable in our dataset. Simply typing \texttt{summary(medv)} or \texttt{summary(lstat)} will give an error message because we have not told R to look in the Boston dataset for these variables. To refer to a variable, we must type the dataset name followed by a \$ symbol and the variable name.

\begin{verbatim}
> summary(Boston$medv)
Min. 1st Qu. Median Mean 3rd Qu. Max.
5.00 17.02 21.20 22.53 25.00 50.00
\end{verbatim}

Alternatively, use the \texttt{attach()} function in order to tell R to make the variables in the attached dataset available by name. It is good practice to detach the dataset after the task is completed. To avoid the \texttt{attach()} function, it is recommended to apply the \texttt{with(data...)} function.

\begin{verbatim}
> attach(Boston)
> summary(lstat)
Min. 1st Qu. Median Mean 3rd Qu. Max.
1.73 6.95 11.36 12.65 16.95 37.97
> detach(Boston)

> with(data=Boston, summary(medv))
Min. 1st Qu. Median Mean 3rd Qu. Max.
5.00 17.02 21.20 22.53 25.00 50.00
\end{verbatim}

We can now produce the summary table for the variables of interest. The \texttt{cbind()} function combines by columns the numerical summaries of \texttt{medv} and \texttt{lstat}.

\begin{verbatim}
> attach(Boston)
> summary(cbind(medv, lstat))
medv lstat
Min. : 5.00 Min. : 1.73
1st Qu. : 17.02 1st Qu. : 6.95
Median : 21.20 Median : 11.36
Mean : 22.53 Mean : 12.65
3rd Qu. : 25.00 3rd Qu. : 16.95
Max. : 50.00 Max. : 37.97
\end{verbatim}

\subsection{Code Summary}

\begin{verbatim}
library(MASS)
names(Boston)
dim(Boston)
head(Boston)
summary(Boston$medv)
attach(Boston)
summary(lstat)
summary(cbind(medv, lstat))
detach(Boston)
\end{verbatim}

This summary code provides a concise reference for the R commands used in the first step of our simple linear regression analysis.

\subsection{Step 2: Boxplot and Scatterplot}

\subsubsection{Comparative Boxplots}

Comparative boxplots are a convenient way of graphically depicting groups of numerical data. They provide a visual representation of the central tendency, spread, and skewness of the data, as well as identify potential outliers. We generate boxplots for our variables of interest, medv and lstat, using the following R commands:

\begin{verbatim}
library(MASS)
attach(Boston)
boxplot(cbind(medv, lstat), horizontal = TRUE)
\end{verbatim}

The boxplots suggest that there may exist some possible outliers affecting our analysis. If the outliers were removed, the distribution of lstat would seem slightly right-skewed and the distribution of medv would seem symmetrical.

\subsubsection{Scatterplot}

The scatterplot here is used to assess the relationship between the variables of interest, medv and lstat. The scatterplot can reveal patterns, trends, relationships, or the absence thereof, between two variables.

\begin{verbatim}
library(MASS)
attach(Boston)
plot(lstat, medv)
\end{verbatim}

We can see that there exists a decreasing relationship between the medv and lstat variables. There seems to be some evidence of non-linearity in this relationship. On the other hand, it is likely that after removing the outliers, the relationship would appear more linear. In this example, we will fit the linear model to the original dataset. However, you can later experiment with removing the outliers from your dataset.

\subsection{Step 3: Fitting the SLR}

The simple linear regression model involves only one independent variable (X). The functional relationship between the true mean of \(Y_i\) (that is \( E(Y_i) \)) and \( X_i \) is the equation of a straight line:

\begin{equation}
    \[\mathbb{E}(Y_i) = \beta_0 + \beta_1X_i\]
\end{equation}

for \(i = 1, \ldots, n\), where:

\begin{itemize}
    \item - \(\beta_0\) - intercept of the line - the value of \(E(Y_i)\) when \(X = 0\)
    \item - \(\beta_1\) - slope of the line - the rate of change in \(E(Y_i)\) per unit change in \(X\)
\end{itemize}

The deviation of the observation from its population mean is taken into account by adding a random error \(\varepsilon_i\) to give the statistical model:

\begin{itemize}
    \[Y_i = \beta_0 + \beta_1X_i + \varepsilon_i\]
\end{itemize}

for \(i = 1, 2, \ldots, n\).

There are two important additional assumptions in the SLR analysis:

\begin{enumerate}
    \item \(X_i\) are measured without error so are fixed constants;
    \item The errors \(\varepsilon_i\) are independent from each other and are normally distributed with a mean of 0 and a common variance \(\sigma^2 \), i.e., \(\varepsilon_i \sim N(0, \sigma^2)\).
\end{enumerate}

Let \(\hat{\beta_0}\) and \(\hat{\beta_1}\) be numerical estimates of the parameters \(\beta_0\) and \(\beta_1\) obtained from data and let

\[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i\]

be the estimated mean of \(Y_i\) or prediction of \(Y_i\) when \(X_i = x_i\) for each \(i = 1, \ldots, n\).

The least squares principle chooses \( \hat{\beta_0} \) and \( \hat{\beta_1} \) that minimize the sum of squares of the residuals. The \(i\)th residual (\( e_i \)) represents the prediction error for data point \( i \) when we use \( \hat{y_i} \) to predict the actual response \( y_i \). The residual sum of squares (RSS) is then given by

\[RSS = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - (\hat{\beta_0} + \hat{\beta_1}x_i))^2\]

The estimates of \(\beta_0\) and \(\beta_1\) are obtained by minimizing RSS. The derivatives of RSS with respect to \(\hat{\beta_0}\) and \(\hat{\beta_1}\) are set to zero and as a result, the least squares estimates must satisfy the following normal equations:

\begin{equation}
    \[\sum_{i=1}^{n} y_i = n\hat{\beta_0} + (\sum_{i=1}^{n} x_i)\hat{\beta_1}\]
    \[\sum_{i=1}^{n} x_i y_i = (\sum_{i=1}^{n} x_i)\hat{\beta_0} + (\sum_{i=1}^{n} x_i^2)\hat{\beta_1}\]
\end{equation}

Solving the above equations gives the least squares estimates for the slope and intercept:

\begin{equation}
    \[\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\]
    \[\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}\]
\end{equation}

where \(\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \) and \( \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i\) are the sample means.

The estimates from give the equation of the best fitting line:

\[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i\]

Naturally, we still have to verify whether \(\hat{\beta_0}\) and \(\hat{\beta_1}\) really minimize RSS and satisfy conditions of the minimizing problem. Thus we need the second derivatives of RSS with respect to \(\hat{\beta_0}\) and \(\hat{\beta_1}\), which are given by the so-called Hessian matrix (matrix of second derivatives).

The Hessian matrix \( H \) is given by

\[H = 2 \begin{pmatrix}
n & \sum x_i \\
\sum x_i & \sum x_i^2
\end{pmatrix}\]

Then it remains to be shown that the Hessian matrix is positive definite. Since \(n > 0\) and \[\det(H) = 4(n \sum x_i^2 - (\sum x_i)^2) > 0\] from Hölder inequality, the hessian matrix $H$ is positive definite and therefore \(\hat{\beta_0}\) and \( \hat{\beta_1}\) minimize RSS.

\subsubsection{Fitting the Model}

We now fit the simple linear regression model to the data, with \texttt{medv} as the response and \texttt{lstat} as the predictor. We use the \texttt{lm()} function in R:

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
summary(lm.fit)
\end{verbatim}

From the above R output we can write down the obtained regression equation of the ﬁtted line:

\[\text{medv} = 34.55 - 0.95 \times \text{lstat}\]

Which shows that that the (\textt{medv}) median house value, decreases as the percentage of households with low socioeconomic status (\texttt{lstat}) increases, and the rate of decrease is 0.95

All the information provided when we ﬁt the SLR model to our data by using the \textt{lm()} function is now stored in the object \texttt{lm.fit}.
When we type \texttt{summary(lm.fit)}, it displays the detailed summary information about the fitted model. There are diﬀerent ways to extract information from \texttt{lm.fit}. For example, the \texttt{coef(lm.fit)} provides the estimates of the intercept and slope for the fitted model. We see from the output below that $\hat{\beta_0} = 34.55$ and $\hat{\beta_1} = -0.95$, so that the predictions are obtained by $\hat{y_i} = 34.55 - 0.95x_i$

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
coef(lm.fit)
\end{verbatim}

\[ \text{(Intercept)} = 34.55384 \]
\[ \text{lstat} = -0.9500494 \]

These coefficients are used to make predictions. For example, to predict medv for specific values of lstat:

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
predict(lm.fit, newdata=data.frame(lstat=c(5,10,15)))
\end{verbatim}

This R code will provide the predicted medv values for lstat values of 5, 10, and 15.

We will now add the least squares regression line to the scatterplot:

\begin{verbatim}
library(MASS)
lm.fit<-lm(medv~lstat,data=Boston)
attach(Boston)
plot(lstat,medv)
abline(lm.fit,col='red',lwd=3)
\end{verbatim}

\subsection{Step 4: Assessing the Accuracy of the Coefficient Estimates}
\subsubsection{Standard Errors for Coefficient Estimates}

The coefficient estimates given in Step 3 are unbiased, that is, \( E(\hat{\beta}_0) = \beta_0\) and \(E(\hat{\beta}_1) = \beta_1\). The standard errors of \(\hat{\beta}_0\) and \(\hat{\beta}_1\), written as \(SE(\hat{\beta}_0)\) and \(SE(\hat{\beta}_1)\), can be computed using the following formulas:

\[SE(\hat{\beta}_0)^2 = \frac{\sigma^2}{n} \left( 1 + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)\]

\[SE(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\]

where \(\sigma^2 = \text{Var}(\varepsilon)\).

In general, \(\sigma\) is not known but can be estimated from the data. This estimate is known as the residual standard error and is given by

\[\hat{\sigma} = RSE = \sqrt{\frac{RSS}{n - 2}}\]

When \(\sigma\) is estimated from data, we should write \(SE(\hat{\beta}_0)\) and \(SE(\hat{\beta}_1)\) using an extra \"hat\" but we will not use this extra \"hat\" in our notations.

The standard errors for coefficient estimates in our example are \(SE(\hat{\beta}_0) = 0.5626\) and \(SE(\hat{\beta}_1) = 0.0387\), while the estimate of \(\sigma\) is 6.216 (see \texttt{summary(lm.fit)} output).

\subsubsection{Confidence Interval}

Standard errors can be used to compute a \((1 - \alpha)100\%\) confidence intervals for \(\beta_0\) and \(\beta_1\) as:

\[[\hat{\beta}_k - t_{\alpha/2,n-2}SE(\hat{\beta}_k), \hat{\beta}_k + t_{\alpha/2,n-2}SE(\hat{\beta}_k)]\]

where \(k = 0, 1\) and \(t_{\alpha/2,n-2}\) is the critical value of a Student's t-distribution with \(n - 2\) degrees of freedom.

In order to obtain confidence intervals for \(\beta_0\) and \(\beta_1\), we can use the \texttt{confint()} command in R. The level of confidence is 95\% by default. From the R output, the 95\% confidence interval for the slope is \([-1.026, -0.874]\).

\begin{verbatim}
library(MASS)
lm.fit<-lm(medv~lstat,data=Boston)
confint(lm.fit)
\end{verbatim}

\begin{verbatim}
            2.5 %       97.5 %
(Intercept) 33.448457   35.6592247
lstat       -1.026148   -0.8739505
\end{verbatim}

You can also try to use the conﬁdence interval formula above. There are $n=506$ observations in the Boston dataset. The critical value $t_{1 - \alpha / 2,n-2}$ required for the 95\% confidence interval, that is, for $\alpha=0.05$ can be found using the quantile function for the $t$ distribution \texttt{qt()}

\begin{verbatim}
    qt(0.025,504,lower.tail=FALSE)
\end{verbatim}

\begin{verbatim}
    1.964682
\end{verbatim}

\subsubsection{Hypothesis Tests on the Coefficients}

Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of

\[H_0: \beta_1 = 0 \quad (\text{there is no relationship between X and Y})\]

versus the alternative hypothesis

\[H_1: \beta_1 \neq 0 \quad (\text{there is some relationship between X and Y}).\]

For the purpose of testing the above hypothesis, we compute a t-statistic given by

\[t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}\]

which has a Student's t-distribution with \(n - 2\) degrees of freedom under \(H_0\) that there is no relationship between \(X\) and \(Y\).

When \(|t| < t_{\alpha/2,n-2}\), we cannot reject the null hypothesis \(H_0\) at the level of significance \(\alpha\). This is equivalent to the decision based on a p-value, that is, we reject \(H_0\) if the p-value is small enough (p-value < \(\alpha\)).

When \(|t| < t_{\alpha/2,n-2}\), we cannot reject the null hypothesis \(H_0\) at the level of significance \(\alpha\). This is equivalent to the decision based on a p-value, that is, we reject \(H_0\) if the p-value is small enough (p-value < \(\alpha\)).

This is equivalent to the decision based on a \textbf{p-value}, that is, we reject $H_0$ if p-value is small enough (ie p-value $< \alpha$)

In our example (see `summary(lm.fit)` output), at the \(\alpha = 0.05\) level of significance, we can reject the null hypothesis of \( \beta_1 = 0 \) since the p-value < 0.05. The observed value of the t-statistic is -24.53, and the distribution of the test statistic is a Student's t-distribution with 504 degrees of freedom.

Note that p-values of the tests \(H_0: \beta_k = 0\) vs \(H_1: \beta_k \neq 0\) (where \(k = 0, 1\)) are given in the column of Pr(> |t|).

Note also that the F statistic given in the R output corresponds to $H_0$: there is no relationship between the response and the predictor $(\beta_1 = 0)$

\subsection{Step 5: Assessing the Accuracy of the SLR Model}

The quality of a linear regression fit is typically assessed using the residual standard error (RSE) and the $R^2$ statistic

\subsubsection{Residual Standard Error (RSE)}

Recall that the \textbf{residual standard error} (RSE) is an estimate of the standard deviation of \(\varepsilon\) and is given by

\[RSE = \sqrt{\frac{RSS}{n - 2}} = \sqrt{\dfrac{1}{n-2} \sum_{i=1}^{n} (y_i - \hat{y_1})^2}\]

The RSE is considered a measure of lack of fit of the model to the data. It is useful when comparing fits from different models. The RSE will be small for the model which fits the data well since the predictions \( \hat{y}_i \) will be close to the observations \( y_i \), resulting in a small RSE.

The residual standard error in our example is 6.216, which is rather large considering that the median value of medv is 21.2. This suggests that the model does not fit the data well.

\subsubsection{Coefficient of Determination (\( R^2 \))}

The coefficient of determination, \(R^2\), is a measure of the contribution of the independent variable(s) in the model is the $R^2$ statistic, known as the coefficient of determination. It is defined as

\[R^2 = \dfrac{TSS - RSS}{TSS}\]

where \(TSS\) (Total Sum of Squares) is given by

\[TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2\]

and \(RSS\) (Residual Sum of Squares) is given by

\( RSS = \sum_{i=1}^{n} (y_i - \hat{y_i})^2\)

The coefficient of determination can be interpreted in the following way:

\begin{itemize}
    \item TSS is the amount of variability inherent in the response before the regression is performed;
    \item RSS is the amount of variability that is left unexplained after performing the regression;
    \item \(TSS - RSS\) is the amount of variability in the response that is explained (or removed) by performing the regression;
    \item \(R^2\) is the proportion of variability in \(Y\) that can be explained using \(X\); its value always lies between 0 and 1.
    \item In the simple linear regression setting, \(R^2\) is equal to \(r^2\), where \(r\) is the correlation coefficient.
\end{itemize}

The \(R^2\) statistic is rather low in our example (\(R^2 = 0.5441\)), suggesting that only 54\% of the variability in \(medv\) can be explained using \(lstat\). More complex models or removing outliers might be needed to improve \(R^2\).

\subsection{Step 6: Diagnostic Plots}

Several potential problems may arise in linear regression. Below we list such possible issues and suggest some diagnostic plots that can be used to identify them:

\begin{enumerate}
    \item \textbf{Non-linearity of the response-predictor relationship}: residuals plots - we plot residuals $e_i = y_i - \hat{y_i}$ versus \(x_i\) or fitted values \(\hat{y}_i\). Non-linearity can be seen in the presence of a pattern such as a U-shape.
    \item \textbf{Correlation of error terms}: If there is a time component in the data, we plot the residuals as a function of time (when data is time-dependent).
    \item \textbf{Non-constant variance of error terms}: Residual plots can reveal heteroscedasticity, which appears as a funnel shape in the residuals versus fitted values plot.
    \item \textbf{Outliers}: Outliers are observations for which the response is unusually far from the predicted value. Use a plot of studentized residuals computed by dividing each residual by its estimated standard error (RSE). Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.
    \item \textbf{High-leverage points}: Observations with high-leverage have an unusual value for \(x_i\). Plot studentized residuals versus the leverage statistic, defined as

    \item \( h_i = \dfrac{1}{n} + \dfrac{(x_i - \bar{x})^2}{\sum_{k=1}^{n}(x_k - \bar{x})^2}\),

    The leverage statistic has values between $\dfrac{1}{n}$ and $1$ with average $\dfrac{2}{n}$. If given observation has $h_i$ that exceeds 2 or 3 times the average $\dfrac{2}{n}$, then we may suspect the corresponding point has leverage

    \item \textbf{Collinearity}: refers to the situation in which two or more predictor variables are closely related to one another (not the case in SLR).
\end{enumerate}

Diagnostic plots can be produced by applying the `plot()` function directly on the output from `lm()`. We will now plot mdev and lstat along with the least squares regression line:

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
attach(Boston)
plot(lstat, mdev)
abline(lm.fit, col='red', lwd=3)
\end{verbatim}

These plots can help identify potential problems and guide further steps in the analysis.

There is some evidence of non-linearity in the relationship between medv and lstat. Four diagnostic plots can be produced by applying the `plot()` function directly on the output from `lm()`:

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
plot(lm.fit)
\end{verbatim}

On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the `hatvalues()` function:

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
plot(hatvalues(lm.fit))
lines(rep(3*2/506,506),col=2)
\end{verbatim}

The red line in the plot indicates the $3 \times 2/n$ level. To identify the index of the largest element of a vector of
leverage statistics we used the function `which.max()`.

\begin{verbatim}
library(MASS)
lm.fit<-lm(medv~lstat,data=Boston)
which.max(hatvalues(lm.fit))
\end{verbatim}

\begin{verbatim}
375
375
\end{verbatim}

Note also that we can use the `names()` function to show what other information is stored in `lm.fit`:

\begin{verbatim}
library(MASS)
lm.fit<-lm(medv~lstat,data=Boston)
\end{verbatim}

\begin{verbatim}
names(lm.fit)

lm.fit$coefficients
\end{verbatim}

\begin{verbatim}
[1] "coefficients" "residuals" "effects" "rank"
[5] "fitted.values" "assign" "qr" "df.residual"
[9] "xlevels" "call" "terms" "model"
(Intercept)    lstat
34.5538409 -0.9500494
\end{verbatim}

\subsection{Step 7: Prediction}

Let us now use the \texttt{predict()} function to calculate the predicted value of \texttt{medv} for \texttt{lstat} equal to 5, 10, and 15. For example, the predicted value of \texttt{medv} is 25.05 when \texttt{lstat} equals 10.

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
predict(lm.fit, data.frame(lstat=c(5,10,15)))
\end{verbatim}

The output is:

\begin{verbatim}
       1        2        3
29.80359 25.05335 20.30310
\end{verbatim}

The \texttt{predict()} function can also be used to produce confidence intervals and prediction intervals for the predicted value of \texttt{medv} for \texttt{lstat} equal to 5, 10, and 15. For example, the 95\% confidence interval associated with an \texttt{lstat} value of 10 is \([24.47, 25.63]\) and the 95\% prediction interval is \([12.83, 37.28]\). Both intervals are centered around 25.05, which is the predicted value of \texttt{medv} when \texttt{lstat} equals 10.

\begin{verbatim}
library(MASS)
lm.fit <- lm(medv ~ lstat, data=Boston)
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="prediction")
\end{verbatim}

The output is:

\begin{verbatim}
        fit       lwr       upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461

        fit       lwr       upr
1 29.80359 17.565675 42.04151
2 25.05335 12.827626 37.27907
3 20.30310  8.077742 32.52846
\end{verbatim}

This section outlines how to use the \texttt{predict()} function in R for making predictions with a simple linear regression model, including generating confidence and prediction intervals. Let me know if you need further transcription or have any questions about this section.

\end{document}
